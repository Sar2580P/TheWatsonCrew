[
    {
        "heading": "Data Augmentation",
        "content": "### Training Time Augmentation\n- Improves results by virtually increasing training dataset size.\n- Regularization technique, making model robust to input data changes.\n\n",
        "metadata": {
            "external_references": {
                "": "",
                "Press": "pressinquiries@medium.com?source=post_page-----4ac19b67fb4d--------------------------------"
            },
            "sources": [
                "https://jalammar.github.io/illustrated-transformer/",
                "https://towardsdatascience.com/test-time-augmentation-tta-and-how-to-perform-it-with-keras-4ac19b67fb4d"
            ],
            "imgs": [
                "",
                "https://jalammar.github.io/images/t/output_target_probability_distributions.png",
                "https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png",
                "https://jalammar.github.io/images/t/The_transformer_encoders_decoders.png",
                "https://jalammar.github.io/images/t/transformer_attention_heads_weight_matrix_o.png",
                "https://jalammar.github.io/images/t/self-attention-matrix-calculation.png",
                "https://jalammar.github.io/images/t/transformer_self-attention_visualization_3.png",
                "https://jalammar.github.io/images/t/transformer_positional_encoding_large_example.png",
                "https://miro.medium.com/v2/resize:fill:144:144/2*DW37pyGS3g1xXe5_wOB1nw.png",
                "https://jalammar.github.io/images/t/encoder_with_tensors_2.png",
                "https://jalammar.github.io/images/t/Transformer_encoder.png",
                "https://jalammar.github.io/images/t/transformer_attention_heads_qkv.png",
                "https://jalammar.github.io/images/t/transformer_self-attention_visualization.png",
                "https://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png",
                "https://jalammar.github.io/images/t/self-attention-output.png",
                "https://jalammar.github.io/images/t/encoder_with_tensors.png",
                "https://jalammar.github.io/images/t/transformer_self_attention_vectors.png",
                "https://miro.medium.com/v2/resize:fill:64:64/1*CJe3891yB1A1mzMdqemkdg.jpeg",
                "https://jalammar.github.io/images/t/transformer_self-attention_visualization_2.png",
                "https://jalammar.github.io/images/t/transformer_positional_encoding_example.png",
                "https://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png",
                "https://jalammar.github.io/images/t/transformer_positional_encoding_vectors.png",
                "https://jalammar.github.io/images/t/transformer_decoding_2.gif",
                "https://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png",
                "https://jalammar.github.io/images/t/embeddings.png",
                "https://jalammar.github.io/images/t/output_trained_model_probability_distributions.png",
                "https://jalammar.github.io/images/t/attention-is-all-you-need-positional-encoding.png",
                "https://jalammar.github.io/images/t/transformer_logits_output_and_label.png",
                "https://jalammar.github.io/images/t/transformer_decoder_output_softmax.png",
                "https://jalammar.github.io/images/t/the_transformer_3.png",
                "https://jalammar.github.io/images/t/vocabulary.png",
                "https://jalammar.github.io/images/t/Transformer_decoder.png",
                "https://jalammar.github.io/images/t/transformer_attention_heads_z.png",
                "https://jalammar.github.io/images/t/transformer_self_attention_score.png",
                "https://jalammar.github.io/images/t/transformer_resideual_layer_norm_2.png",
                "https://jalammar.github.io/images/t/one-hot-vocabulary-example.png",
                "https://jalammar.github.io/images/t/self-attention_softmax.png",
                "https://jalammar.github.io/images/t/transformer_decoding_1.gif",
                "https://jalammar.github.io/images/t/The_transformer_encoder_decoder_stack.png"
            ]
        }
    },
    {
        "heading": "Data Augmentation",
        "content": "### Test Time Augmentation (TTA)\n- Improves results by changing testing methods.",
        "metadata": {
            "external_references": {
                "": "",
                "Press": "pressinquiries@medium.com?source=post_page-----4ac19b67fb4d--------------------------------"
            },
            "sources": [
                "https://jalammar.github.io/illustrated-transformer/",
                "https://towardsdatascience.com/test-time-augmentation-tta-and-how-to-perform-it-with-keras-4ac19b67fb4d"
            ],
            "imgs": [
                "",
                "https://jalammar.github.io/images/t/output_target_probability_distributions.png",
                "https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png",
                "https://jalammar.github.io/images/t/The_transformer_encoders_decoders.png",
                "https://jalammar.github.io/images/t/transformer_attention_heads_weight_matrix_o.png",
                "https://jalammar.github.io/images/t/self-attention-matrix-calculation.png",
                "https://jalammar.github.io/images/t/transformer_self-attention_visualization_3.png",
                "https://jalammar.github.io/images/t/transformer_positional_encoding_large_example.png",
                "https://miro.medium.com/v2/resize:fill:144:144/2*DW37pyGS3g1xXe5_wOB1nw.png",
                "https://jalammar.github.io/images/t/encoder_with_tensors_2.png",
                "https://jalammar.github.io/images/t/Transformer_encoder.png",
                "https://jalammar.github.io/images/t/transformer_attention_heads_qkv.png",
                "https://jalammar.github.io/images/t/transformer_self-attention_visualization.png",
                "https://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png",
                "https://jalammar.github.io/images/t/self-attention-output.png",
                "https://jalammar.github.io/images/t/encoder_with_tensors.png",
                "https://jalammar.github.io/images/t/transformer_self_attention_vectors.png",
                "https://miro.medium.com/v2/resize:fill:64:64/1*CJe3891yB1A1mzMdqemkdg.jpeg",
                "https://jalammar.github.io/images/t/transformer_self-attention_visualization_2.png",
                "https://jalammar.github.io/images/t/transformer_positional_encoding_example.png",
                "https://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png",
                "https://jalammar.github.io/images/t/transformer_positional_encoding_vectors.png",
                "https://jalammar.github.io/images/t/transformer_decoding_2.gif",
                "https://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png",
                "https://jalammar.github.io/images/t/embeddings.png",
                "https://jalammar.github.io/images/t/output_trained_model_probability_distributions.png",
                "https://jalammar.github.io/images/t/attention-is-all-you-need-positional-encoding.png",
                "https://jalammar.github.io/images/t/transformer_logits_output_and_label.png",
                "https://jalammar.github.io/images/t/transformer_decoder_output_softmax.png",
                "https://jalammar.github.io/images/t/the_transformer_3.png",
                "https://jalammar.github.io/images/t/vocabulary.png",
                "https://jalammar.github.io/images/t/Transformer_decoder.png",
                "https://jalammar.github.io/images/t/transformer_attention_heads_z.png",
                "https://jalammar.github.io/images/t/transformer_self_attention_score.png",
                "https://jalammar.github.io/images/t/transformer_resideual_layer_norm_2.png",
                "https://jalammar.github.io/images/t/one-hot-vocabulary-example.png",
                "https://jalammar.github.io/images/t/self-attention_softmax.png",
                "https://jalammar.github.io/images/t/transformer_decoding_1.gif",
                "https://jalammar.github.io/images/t/The_transformer_encoder_decoder_stack.png"
            ]
        }
    },
    {
        "heading": "Example of TTA",
        "content": "- Neural network trained on CIFAR10 dataset.\n- Test image:\n- True label: Boat (class 9)\n- Model prediction: Car (class 2)\n- Mean of 5 predictions: Correctly predicts Boat",
        "metadata": {
            "external_references": {
                "": "",
                "Press": "pressinquiries@medium.com?source=post_page-----4ac19b67fb4d--------------------------------"
            },
            "sources": [
                "https://jalammar.github.io/illustrated-transformer/",
                "https://towardsdatascience.com/test-time-augmentation-tta-and-how-to-perform-it-with-keras-4ac19b67fb4d"
            ],
            "imgs": [
                "",
                "https://jalammar.github.io/images/t/output_target_probability_distributions.png",
                "https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png",
                "https://jalammar.github.io/images/t/The_transformer_encoders_decoders.png",
                "https://jalammar.github.io/images/t/transformer_attention_heads_weight_matrix_o.png",
                "https://jalammar.github.io/images/t/self-attention-matrix-calculation.png",
                "https://jalammar.github.io/images/t/transformer_self-attention_visualization_3.png",
                "https://jalammar.github.io/images/t/transformer_positional_encoding_large_example.png",
                "https://miro.medium.com/v2/resize:fill:144:144/2*DW37pyGS3g1xXe5_wOB1nw.png",
                "https://jalammar.github.io/images/t/encoder_with_tensors_2.png",
                "https://jalammar.github.io/images/t/Transformer_encoder.png",
                "https://jalammar.github.io/images/t/transformer_attention_heads_qkv.png",
                "https://jalammar.github.io/images/t/transformer_self-attention_visualization.png",
                "https://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png",
                "https://jalammar.github.io/images/t/self-attention-output.png",
                "https://jalammar.github.io/images/t/encoder_with_tensors.png",
                "https://jalammar.github.io/images/t/transformer_self_attention_vectors.png",
                "https://miro.medium.com/v2/resize:fill:64:64/1*CJe3891yB1A1mzMdqemkdg.jpeg",
                "https://jalammar.github.io/images/t/transformer_self-attention_visualization_2.png",
                "https://jalammar.github.io/images/t/transformer_positional_encoding_example.png",
                "https://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png",
                "https://jalammar.github.io/images/t/transformer_positional_encoding_vectors.png",
                "https://jalammar.github.io/images/t/transformer_decoding_2.gif",
                "https://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png",
                "https://jalammar.github.io/images/t/embeddings.png",
                "https://jalammar.github.io/images/t/output_trained_model_probability_distributions.png",
                "https://jalammar.github.io/images/t/attention-is-all-you-need-positional-encoding.png",
                "https://jalammar.github.io/images/t/transformer_logits_output_and_label.png",
                "https://jalammar.github.io/images/t/transformer_decoder_output_softmax.png",
                "https://jalammar.github.io/images/t/the_transformer_3.png",
                "https://jalammar.github.io/images/t/vocabulary.png",
                "https://jalammar.github.io/images/t/Transformer_decoder.png",
                "https://jalammar.github.io/images/t/transformer_attention_heads_z.png",
                "https://jalammar.github.io/images/t/transformer_self_attention_score.png",
                "https://jalammar.github.io/images/t/transformer_resideual_layer_norm_2.png",
                "https://jalammar.github.io/images/t/one-hot-vocabulary-example.png",
                "https://jalammar.github.io/images/t/self-attention_softmax.png",
                "https://jalammar.github.io/images/t/transformer_decoding_1.gif",
                "https://jalammar.github.io/images/t/The_transformer_encoder_decoder_stack.png"
            ]
        }
    },
    {
        "heading": "Using TTA with Keras",
        "content": "- Create a simple Convolutional Neural Network.\n- Train the network on CIFAR10 dataset.",
        "metadata": {
            "external_references": {
                "": "",
                "Press": "pressinquiries@medium.com?source=post_page-----4ac19b67fb4d--------------------------------"
            },
            "sources": [
                "https://jalammar.github.io/illustrated-transformer/",
                "https://towardsdatascience.com/test-time-augmentation-tta-and-how-to-perform-it-with-keras-4ac19b67fb4d"
            ],
            "imgs": [
                "",
                "https://jalammar.github.io/images/t/output_target_probability_distributions.png",
                "https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png",
                "https://jalammar.github.io/images/t/The_transformer_encoders_decoders.png",
                "https://jalammar.github.io/images/t/transformer_attention_heads_weight_matrix_o.png",
                "https://jalammar.github.io/images/t/self-attention-matrix-calculation.png",
                "https://jalammar.github.io/images/t/transformer_self-attention_visualization_3.png",
                "https://jalammar.github.io/images/t/transformer_positional_encoding_large_example.png",
                "https://miro.medium.com/v2/resize:fill:144:144/2*DW37pyGS3g1xXe5_wOB1nw.png",
                "https://jalammar.github.io/images/t/encoder_with_tensors_2.png",
                "https://jalammar.github.io/images/t/Transformer_encoder.png",
                "https://jalammar.github.io/images/t/transformer_attention_heads_qkv.png",
                "https://jalammar.github.io/images/t/transformer_self-attention_visualization.png",
                "https://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png",
                "https://jalammar.github.io/images/t/self-attention-output.png",
                "https://jalammar.github.io/images/t/encoder_with_tensors.png",
                "https://jalammar.github.io/images/t/transformer_self_attention_vectors.png",
                "https://miro.medium.com/v2/resize:fill:64:64/1*CJe3891yB1A1mzMdqemkdg.jpeg",
                "https://jalammar.github.io/images/t/transformer_self-attention_visualization_2.png",
                "https://jalammar.github.io/images/t/transformer_positional_encoding_example.png",
                "https://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png",
                "https://jalammar.github.io/images/t/transformer_positional_encoding_vectors.png",
                "https://jalammar.github.io/images/t/transformer_decoding_2.gif",
                "https://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png",
                "https://jalammar.github.io/images/t/embeddings.png",
                "https://jalammar.github.io/images/t/output_trained_model_probability_distributions.png",
                "https://jalammar.github.io/images/t/attention-is-all-you-need-positional-encoding.png",
                "https://jalammar.github.io/images/t/transformer_logits_output_and_label.png",
                "https://jalammar.github.io/images/t/transformer_decoder_output_softmax.png",
                "https://jalammar.github.io/images/t/the_transformer_3.png",
                "https://jalammar.github.io/images/t/vocabulary.png",
                "https://jalammar.github.io/images/t/Transformer_decoder.png",
                "https://jalammar.github.io/images/t/transformer_attention_heads_z.png",
                "https://jalammar.github.io/images/t/transformer_self_attention_score.png",
                "https://jalammar.github.io/images/t/transformer_resideual_layer_norm_2.png",
                "https://jalammar.github.io/images/t/one-hot-vocabulary-example.png",
                "https://jalammar.github.io/images/t/self-attention_softmax.png",
                "https://jalammar.github.io/images/t/transformer_decoding_1.gif",
                "https://jalammar.github.io/images/t/The_transformer_encoder_decoder_stack.png"
            ]
        }
    },
    {
        "heading": "Cautions for Data Augmentation",
        "content": "- Use wisely to avoid hurting model accuracy.\n- Consider data type when choosing augmentation techniques.\n- Example:\n    - MNIST: Avoid random flips or large rotations.\n    - CIFAR10: Horizontal flips are appropriate.\n    - Satellite imaging: Large rotations and vertical flips can be used.",
        "metadata": {
            "external_references": {
                "": "",
                "Press": "pressinquiries@medium.com?source=post_page-----4ac19b67fb4d--------------------------------"
            },
            "sources": [
                "https://jalammar.github.io/illustrated-transformer/",
                "https://towardsdatascience.com/test-time-augmentation-tta-and-how-to-perform-it-with-keras-4ac19b67fb4d"
            ],
            "imgs": [
                "",
                "https://jalammar.github.io/images/t/output_target_probability_distributions.png",
                "https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png",
                "https://jalammar.github.io/images/t/The_transformer_encoders_decoders.png",
                "https://jalammar.github.io/images/t/transformer_attention_heads_weight_matrix_o.png",
                "https://jalammar.github.io/images/t/self-attention-matrix-calculation.png",
                "https://jalammar.github.io/images/t/transformer_self-attention_visualization_3.png",
                "https://jalammar.github.io/images/t/transformer_positional_encoding_large_example.png",
                "https://miro.medium.com/v2/resize:fill:144:144/2*DW37pyGS3g1xXe5_wOB1nw.png",
                "https://jalammar.github.io/images/t/encoder_with_tensors_2.png",
                "https://jalammar.github.io/images/t/Transformer_encoder.png",
                "https://jalammar.github.io/images/t/transformer_attention_heads_qkv.png",
                "https://jalammar.github.io/images/t/transformer_self-attention_visualization.png",
                "https://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png",
                "https://jalammar.github.io/images/t/self-attention-output.png",
                "https://jalammar.github.io/images/t/encoder_with_tensors.png",
                "https://jalammar.github.io/images/t/transformer_self_attention_vectors.png",
                "https://miro.medium.com/v2/resize:fill:64:64/1*CJe3891yB1A1mzMdqemkdg.jpeg",
                "https://jalammar.github.io/images/t/transformer_self-attention_visualization_2.png",
                "https://jalammar.github.io/images/t/transformer_positional_encoding_example.png",
                "https://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png",
                "https://jalammar.github.io/images/t/transformer_positional_encoding_vectors.png",
                "https://jalammar.github.io/images/t/transformer_decoding_2.gif",
                "https://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png",
                "https://jalammar.github.io/images/t/embeddings.png",
                "https://jalammar.github.io/images/t/output_trained_model_probability_distributions.png",
                "https://jalammar.github.io/images/t/attention-is-all-you-need-positional-encoding.png",
                "https://jalammar.github.io/images/t/transformer_logits_output_and_label.png",
                "https://jalammar.github.io/images/t/transformer_decoder_output_softmax.png",
                "https://jalammar.github.io/images/t/the_transformer_3.png",
                "https://jalammar.github.io/images/t/vocabulary.png",
                "https://jalammar.github.io/images/t/Transformer_decoder.png",
                "https://jalammar.github.io/images/t/transformer_attention_heads_z.png",
                "https://jalammar.github.io/images/t/transformer_self_attention_score.png",
                "https://jalammar.github.io/images/t/transformer_resideual_layer_norm_2.png",
                "https://jalammar.github.io/images/t/one-hot-vocabulary-example.png",
                "https://jalammar.github.io/images/t/self-attention_softmax.png",
                "https://jalammar.github.io/images/t/transformer_decoding_1.gif",
                "https://jalammar.github.io/images/t/The_transformer_encoder_decoder_stack.png"
            ]
        }
    },
    {
        "heading": "Conclusion",
        "content": "- Data augmentation can boost model results at both training and testing time.\n- Use wisely to maximize benefits.",
        "metadata": {
            "external_references": {
                "": "",
                "Press": "pressinquiries@medium.com?source=post_page-----4ac19b67fb4d--------------------------------"
            },
            "sources": [
                "https://jalammar.github.io/illustrated-transformer/",
                "https://towardsdatascience.com/test-time-augmentation-tta-and-how-to-perform-it-with-keras-4ac19b67fb4d"
            ],
            "imgs": [
                "",
                "https://jalammar.github.io/images/t/output_target_probability_distributions.png",
                "https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png",
                "https://jalammar.github.io/images/t/The_transformer_encoders_decoders.png",
                "https://jalammar.github.io/images/t/transformer_attention_heads_weight_matrix_o.png",
                "https://jalammar.github.io/images/t/self-attention-matrix-calculation.png",
                "https://jalammar.github.io/images/t/transformer_self-attention_visualization_3.png",
                "https://jalammar.github.io/images/t/transformer_positional_encoding_large_example.png",
                "https://miro.medium.com/v2/resize:fill:144:144/2*DW37pyGS3g1xXe5_wOB1nw.png",
                "https://jalammar.github.io/images/t/encoder_with_tensors_2.png",
                "https://jalammar.github.io/images/t/Transformer_encoder.png",
                "https://jalammar.github.io/images/t/transformer_attention_heads_qkv.png",
                "https://jalammar.github.io/images/t/transformer_self-attention_visualization.png",
                "https://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png",
                "https://jalammar.github.io/images/t/self-attention-output.png",
                "https://jalammar.github.io/images/t/encoder_with_tensors.png",
                "https://jalammar.github.io/images/t/transformer_self_attention_vectors.png",
                "https://miro.medium.com/v2/resize:fill:64:64/1*CJe3891yB1A1mzMdqemkdg.jpeg",
                "https://jalammar.github.io/images/t/transformer_self-attention_visualization_2.png",
                "https://jalammar.github.io/images/t/transformer_positional_encoding_example.png",
                "https://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png",
                "https://jalammar.github.io/images/t/transformer_positional_encoding_vectors.png",
                "https://jalammar.github.io/images/t/transformer_decoding_2.gif",
                "https://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png",
                "https://jalammar.github.io/images/t/embeddings.png",
                "https://jalammar.github.io/images/t/output_trained_model_probability_distributions.png",
                "https://jalammar.github.io/images/t/attention-is-all-you-need-positional-encoding.png",
                "https://jalammar.github.io/images/t/transformer_logits_output_and_label.png",
                "https://jalammar.github.io/images/t/transformer_decoder_output_softmax.png",
                "https://jalammar.github.io/images/t/the_transformer_3.png",
                "https://jalammar.github.io/images/t/vocabulary.png",
                "https://jalammar.github.io/images/t/Transformer_decoder.png",
                "https://jalammar.github.io/images/t/transformer_attention_heads_z.png",
                "https://jalammar.github.io/images/t/transformer_self_attention_score.png",
                "https://jalammar.github.io/images/t/transformer_resideual_layer_norm_2.png",
                "https://jalammar.github.io/images/t/one-hot-vocabulary-example.png",
                "https://jalammar.github.io/images/t/self-attention_softmax.png",
                "https://jalammar.github.io/images/t/transformer_decoding_1.gif",
                "https://jalammar.github.io/images/t/The_transformer_encoder_decoder_stack.png"
            ]
        }
    },
    {
        "heading": "The Transformer",
        "content": "### Overview\n- Model that uses attention to improve speed and performance of neural machine translation.\n- Outperforms Google Neural Machine Translation model.\n- Recommended by Google Cloud for use with Cloud TPU.\n\n",
        "metadata": {
            "external_references": {
                "": "",
                "Press": "pressinquiries@medium.com?source=post_page-----4ac19b67fb4d--------------------------------"
            },
            "sources": [
                "https://jalammar.github.io/illustrated-transformer/",
                "https://towardsdatascience.com/test-time-augmentation-tta-and-how-to-perform-it-with-keras-4ac19b67fb4d"
            ],
            "imgs": [
                "",
                "https://jalammar.github.io/images/t/output_target_probability_distributions.png",
                "https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png",
                "https://jalammar.github.io/images/t/The_transformer_encoders_decoders.png",
                "https://jalammar.github.io/images/t/transformer_attention_heads_weight_matrix_o.png",
                "https://jalammar.github.io/images/t/self-attention-matrix-calculation.png",
                "https://jalammar.github.io/images/t/transformer_self-attention_visualization_3.png",
                "https://jalammar.github.io/images/t/transformer_positional_encoding_large_example.png",
                "https://miro.medium.com/v2/resize:fill:144:144/2*DW37pyGS3g1xXe5_wOB1nw.png",
                "https://jalammar.github.io/images/t/encoder_with_tensors_2.png",
                "https://jalammar.github.io/images/t/Transformer_encoder.png",
                "https://jalammar.github.io/images/t/transformer_attention_heads_qkv.png",
                "https://jalammar.github.io/images/t/transformer_self-attention_visualization.png",
                "https://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png",
                "https://jalammar.github.io/images/t/self-attention-output.png",
                "https://jalammar.github.io/images/t/encoder_with_tensors.png",
                "https://jalammar.github.io/images/t/transformer_self_attention_vectors.png",
                "https://miro.medium.com/v2/resize:fill:64:64/1*CJe3891yB1A1mzMdqemkdg.jpeg",
                "https://jalammar.github.io/images/t/transformer_self-attention_visualization_2.png",
                "https://jalammar.github.io/images/t/transformer_positional_encoding_example.png",
                "https://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png",
                "https://jalammar.github.io/images/t/transformer_positional_encoding_vectors.png",
                "https://jalammar.github.io/images/t/transformer_decoding_2.gif",
                "https://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png",
                "https://jalammar.github.io/images/t/embeddings.png",
                "https://jalammar.github.io/images/t/output_trained_model_probability_distributions.png",
                "https://jalammar.github.io/images/t/attention-is-all-you-need-positional-encoding.png",
                "https://jalammar.github.io/images/t/transformer_logits_output_and_label.png",
                "https://jalammar.github.io/images/t/transformer_decoder_output_softmax.png",
                "https://jalammar.github.io/images/t/the_transformer_3.png",
                "https://jalammar.github.io/images/t/vocabulary.png",
                "https://jalammar.github.io/images/t/Transformer_decoder.png",
                "https://jalammar.github.io/images/t/transformer_attention_heads_z.png",
                "https://jalammar.github.io/images/t/transformer_self_attention_score.png",
                "https://jalammar.github.io/images/t/transformer_resideual_layer_norm_2.png",
                "https://jalammar.github.io/images/t/one-hot-vocabulary-example.png",
                "https://jalammar.github.io/images/t/self-attention_softmax.png",
                "https://jalammar.github.io/images/t/transformer_decoding_1.gif",
                "https://jalammar.github.io/images/t/The_transformer_encoder_decoder_stack.png"
            ]
        }
    },
    {
        "heading": "The Transformer",
        "content": "### Components\n- Encoding component: Stack of encoders.\n- Decoding component: Stack of decoders.\n- Connections between encoders and decoders.\n\n",
        "metadata": {
            "external_references": {
                "": "",
                "Press": "pressinquiries@medium.com?source=post_page-----4ac19b67fb4d--------------------------------"
            },
            "sources": [
                "https://jalammar.github.io/illustrated-transformer/",
                "https://towardsdatascience.com/test-time-augmentation-tta-and-how-to-perform-it-with-keras-4ac19b67fb4d"
            ],
            "imgs": [
                "",
                "https://jalammar.github.io/images/t/output_target_probability_distributions.png",
                "https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png",
                "https://jalammar.github.io/images/t/The_transformer_encoders_decoders.png",
                "https://jalammar.github.io/images/t/transformer_attention_heads_weight_matrix_o.png",
                "https://jalammar.github.io/images/t/self-attention-matrix-calculation.png",
                "https://jalammar.github.io/images/t/transformer_self-attention_visualization_3.png",
                "https://jalammar.github.io/images/t/transformer_positional_encoding_large_example.png",
                "https://miro.medium.com/v2/resize:fill:144:144/2*DW37pyGS3g1xXe5_wOB1nw.png",
                "https://jalammar.github.io/images/t/encoder_with_tensors_2.png",
                "https://jalammar.github.io/images/t/Transformer_encoder.png",
                "https://jalammar.github.io/images/t/transformer_attention_heads_qkv.png",
                "https://jalammar.github.io/images/t/transformer_self-attention_visualization.png",
                "https://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png",
                "https://jalammar.github.io/images/t/self-attention-output.png",
                "https://jalammar.github.io/images/t/encoder_with_tensors.png",
                "https://jalammar.github.io/images/t/transformer_self_attention_vectors.png",
                "https://miro.medium.com/v2/resize:fill:64:64/1*CJe3891yB1A1mzMdqemkdg.jpeg",
                "https://jalammar.github.io/images/t/transformer_self-attention_visualization_2.png",
                "https://jalammar.github.io/images/t/transformer_positional_encoding_example.png",
                "https://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png",
                "https://jalammar.github.io/images/t/transformer_positional_encoding_vectors.png",
                "https://jalammar.github.io/images/t/transformer_decoding_2.gif",
                "https://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png",
                "https://jalammar.github.io/images/t/embeddings.png",
                "https://jalammar.github.io/images/t/output_trained_model_probability_distributions.png",
                "https://jalammar.github.io/images/t/attention-is-all-you-need-positional-encoding.png",
                "https://jalammar.github.io/images/t/transformer_logits_output_and_label.png",
                "https://jalammar.github.io/images/t/transformer_decoder_output_softmax.png",
                "https://jalammar.github.io/images/t/the_transformer_3.png",
                "https://jalammar.github.io/images/t/vocabulary.png",
                "https://jalammar.github.io/images/t/Transformer_decoder.png",
                "https://jalammar.github.io/images/t/transformer_attention_heads_z.png",
                "https://jalammar.github.io/images/t/transformer_self_attention_score.png",
                "https://jalammar.github.io/images/t/transformer_resideual_layer_norm_2.png",
                "https://jalammar.github.io/images/t/one-hot-vocabulary-example.png",
                "https://jalammar.github.io/images/t/self-attention_softmax.png",
                "https://jalammar.github.io/images/t/transformer_decoding_1.gif",
                "https://jalammar.github.io/images/t/The_transformer_encoder_decoder_stack.png"
            ]
        }
    },
    {
        "heading": "The Transformer",
        "content": "### Self-Attention\n- Allows decoder to focus on relevant parts of input sentence.\n- Similar to hidden state in RNNs.\n- Helps model understand relationships between words.\n\n",
        "metadata": {
            "external_references": {
                "": "",
                "Press": "pressinquiries@medium.com?source=post_page-----4ac19b67fb4d--------------------------------"
            },
            "sources": [
                "https://jalammar.github.io/illustrated-transformer/",
                "https://towardsdatascience.com/test-time-augmentation-tta-and-how-to-perform-it-with-keras-4ac19b67fb4d"
            ],
            "imgs": [
                "",
                "https://jalammar.github.io/images/t/output_target_probability_distributions.png",
                "https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png",
                "https://jalammar.github.io/images/t/The_transformer_encoders_decoders.png",
                "https://jalammar.github.io/images/t/transformer_attention_heads_weight_matrix_o.png",
                "https://jalammar.github.io/images/t/self-attention-matrix-calculation.png",
                "https://jalammar.github.io/images/t/transformer_self-attention_visualization_3.png",
                "https://jalammar.github.io/images/t/transformer_positional_encoding_large_example.png",
                "https://miro.medium.com/v2/resize:fill:144:144/2*DW37pyGS3g1xXe5_wOB1nw.png",
                "https://jalammar.github.io/images/t/encoder_with_tensors_2.png",
                "https://jalammar.github.io/images/t/Transformer_encoder.png",
                "https://jalammar.github.io/images/t/transformer_attention_heads_qkv.png",
                "https://jalammar.github.io/images/t/transformer_self-attention_visualization.png",
                "https://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png",
                "https://jalammar.github.io/images/t/self-attention-output.png",
                "https://jalammar.github.io/images/t/encoder_with_tensors.png",
                "https://jalammar.github.io/images/t/transformer_self_attention_vectors.png",
                "https://miro.medium.com/v2/resize:fill:64:64/1*CJe3891yB1A1mzMdqemkdg.jpeg",
                "https://jalammar.github.io/images/t/transformer_self-attention_visualization_2.png",
                "https://jalammar.github.io/images/t/transformer_positional_encoding_example.png",
                "https://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png",
                "https://jalammar.github.io/images/t/transformer_positional_encoding_vectors.png",
                "https://jalammar.github.io/images/t/transformer_decoding_2.gif",
                "https://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png",
                "https://jalammar.github.io/images/t/embeddings.png",
                "https://jalammar.github.io/images/t/output_trained_model_probability_distributions.png",
                "https://jalammar.github.io/images/t/attention-is-all-you-need-positional-encoding.png",
                "https://jalammar.github.io/images/t/transformer_logits_output_and_label.png",
                "https://jalammar.github.io/images/t/transformer_decoder_output_softmax.png",
                "https://jalammar.github.io/images/t/the_transformer_3.png",
                "https://jalammar.github.io/images/t/vocabulary.png",
                "https://jalammar.github.io/images/t/Transformer_decoder.png",
                "https://jalammar.github.io/images/t/transformer_attention_heads_z.png",
                "https://jalammar.github.io/images/t/transformer_self_attention_score.png",
                "https://jalammar.github.io/images/t/transformer_resideual_layer_norm_2.png",
                "https://jalammar.github.io/images/t/one-hot-vocabulary-example.png",
                "https://jalammar.github.io/images/t/self-attention_softmax.png",
                "https://jalammar.github.io/images/t/transformer_decoding_1.gif",
                "https://jalammar.github.io/images/t/The_transformer_encoder_decoder_stack.png"
            ]
        }
    },
    {
        "heading": "The Transformer",
        "content": "### Multi-Headed Attention\n- Expands model's ability to focus on different positions.\n- Gives attention layer multiple \"representation subspaces\".\n\n",
        "metadata": {
            "external_references": {
                "": "",
                "Press": "pressinquiries@medium.com?source=post_page-----4ac19b67fb4d--------------------------------"
            },
            "sources": [
                "https://jalammar.github.io/illustrated-transformer/",
                "https://towardsdatascience.com/test-time-augmentation-tta-and-how-to-perform-it-with-keras-4ac19b67fb4d"
            ],
            "imgs": [
                "",
                "https://jalammar.github.io/images/t/output_target_probability_distributions.png",
                "https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png",
                "https://jalammar.github.io/images/t/The_transformer_encoders_decoders.png",
                "https://jalammar.github.io/images/t/transformer_attention_heads_weight_matrix_o.png",
                "https://jalammar.github.io/images/t/self-attention-matrix-calculation.png",
                "https://jalammar.github.io/images/t/transformer_self-attention_visualization_3.png",
                "https://jalammar.github.io/images/t/transformer_positional_encoding_large_example.png",
                "https://miro.medium.com/v2/resize:fill:144:144/2*DW37pyGS3g1xXe5_wOB1nw.png",
                "https://jalammar.github.io/images/t/encoder_with_tensors_2.png",
                "https://jalammar.github.io/images/t/Transformer_encoder.png",
                "https://jalammar.github.io/images/t/transformer_attention_heads_qkv.png",
                "https://jalammar.github.io/images/t/transformer_self-attention_visualization.png",
                "https://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png",
                "https://jalammar.github.io/images/t/self-attention-output.png",
                "https://jalammar.github.io/images/t/encoder_with_tensors.png",
                "https://jalammar.github.io/images/t/transformer_self_attention_vectors.png",
                "https://miro.medium.com/v2/resize:fill:64:64/1*CJe3891yB1A1mzMdqemkdg.jpeg",
                "https://jalammar.github.io/images/t/transformer_self-attention_visualization_2.png",
                "https://jalammar.github.io/images/t/transformer_positional_encoding_example.png",
                "https://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png",
                "https://jalammar.github.io/images/t/transformer_positional_encoding_vectors.png",
                "https://jalammar.github.io/images/t/transformer_decoding_2.gif",
                "https://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png",
                "https://jalammar.github.io/images/t/embeddings.png",
                "https://jalammar.github.io/images/t/output_trained_model_probability_distributions.png",
                "https://jalammar.github.io/images/t/attention-is-all-you-need-positional-encoding.png",
                "https://jalammar.github.io/images/t/transformer_logits_output_and_label.png",
                "https://jalammar.github.io/images/t/transformer_decoder_output_softmax.png",
                "https://jalammar.github.io/images/t/the_transformer_3.png",
                "https://jalammar.github.io/images/t/vocabulary.png",
                "https://jalammar.github.io/images/t/Transformer_decoder.png",
                "https://jalammar.github.io/images/t/transformer_attention_heads_z.png",
                "https://jalammar.github.io/images/t/transformer_self_attention_score.png",
                "https://jalammar.github.io/images/t/transformer_resideual_layer_norm_2.png",
                "https://jalammar.github.io/images/t/one-hot-vocabulary-example.png",
                "https://jalammar.github.io/images/t/self-attention_softmax.png",
                "https://jalammar.github.io/images/t/transformer_decoding_1.gif",
                "https://jalammar.github.io/images/t/The_transformer_encoder_decoder_stack.png"
            ]
        }
    },
    {
        "heading": "The Transformer",
        "content": "### Positional Encoding\n- Adds vectors to input embeddings to indicate word position.\n- Helps model determine distance between words in sequence.\n\n",
        "metadata": {
            "external_references": {
                "": "",
                "Press": "pressinquiries@medium.com?source=post_page-----4ac19b67fb4d--------------------------------"
            },
            "sources": [
                "https://jalammar.github.io/illustrated-transformer/",
                "https://towardsdatascience.com/test-time-augmentation-tta-and-how-to-perform-it-with-keras-4ac19b67fb4d"
            ],
            "imgs": [
                "",
                "https://jalammar.github.io/images/t/output_target_probability_distributions.png",
                "https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png",
                "https://jalammar.github.io/images/t/The_transformer_encoders_decoders.png",
                "https://jalammar.github.io/images/t/transformer_attention_heads_weight_matrix_o.png",
                "https://jalammar.github.io/images/t/self-attention-matrix-calculation.png",
                "https://jalammar.github.io/images/t/transformer_self-attention_visualization_3.png",
                "https://jalammar.github.io/images/t/transformer_positional_encoding_large_example.png",
                "https://miro.medium.com/v2/resize:fill:144:144/2*DW37pyGS3g1xXe5_wOB1nw.png",
                "https://jalammar.github.io/images/t/encoder_with_tensors_2.png",
                "https://jalammar.github.io/images/t/Transformer_encoder.png",
                "https://jalammar.github.io/images/t/transformer_attention_heads_qkv.png",
                "https://jalammar.github.io/images/t/transformer_self-attention_visualization.png",
                "https://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png",
                "https://jalammar.github.io/images/t/self-attention-output.png",
                "https://jalammar.github.io/images/t/encoder_with_tensors.png",
                "https://jalammar.github.io/images/t/transformer_self_attention_vectors.png",
                "https://miro.medium.com/v2/resize:fill:64:64/1*CJe3891yB1A1mzMdqemkdg.jpeg",
                "https://jalammar.github.io/images/t/transformer_self-attention_visualization_2.png",
                "https://jalammar.github.io/images/t/transformer_positional_encoding_example.png",
                "https://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png",
                "https://jalammar.github.io/images/t/transformer_positional_encoding_vectors.png",
                "https://jalammar.github.io/images/t/transformer_decoding_2.gif",
                "https://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png",
                "https://jalammar.github.io/images/t/embeddings.png",
                "https://jalammar.github.io/images/t/output_trained_model_probability_distributions.png",
                "https://jalammar.github.io/images/t/attention-is-all-you-need-positional-encoding.png",
                "https://jalammar.github.io/images/t/transformer_logits_output_and_label.png",
                "https://jalammar.github.io/images/t/transformer_decoder_output_softmax.png",
                "https://jalammar.github.io/images/t/the_transformer_3.png",
                "https://jalammar.github.io/images/t/vocabulary.png",
                "https://jalammar.github.io/images/t/Transformer_decoder.png",
                "https://jalammar.github.io/images/t/transformer_attention_heads_z.png",
                "https://jalammar.github.io/images/t/transformer_self_attention_score.png",
                "https://jalammar.github.io/images/t/transformer_resideual_layer_norm_2.png",
                "https://jalammar.github.io/images/t/one-hot-vocabulary-example.png",
                "https://jalammar.github.io/images/t/self-attention_softmax.png",
                "https://jalammar.github.io/images/t/transformer_decoding_1.gif",
                "https://jalammar.github.io/images/t/The_transformer_encoder_decoder_stack.png"
            ]
        }
    },
    {
        "heading": "The Transformer",
        "content": "### Decoder\n- Outputs a vector of floats.\n- Linear layer projects vector into logits vector.\n- Softmax layer turns logits into probabilities.\n- Word with highest probability is chosen as output.\n\n",
        "metadata": {
            "external_references": {
                "": "",
                "Press": "pressinquiries@medium.com?source=post_page-----4ac19b67fb4d--------------------------------"
            },
            "sources": [
                "https://jalammar.github.io/illustrated-transformer/",
                "https://towardsdatascience.com/test-time-augmentation-tta-and-how-to-perform-it-with-keras-4ac19b67fb4d"
            ],
            "imgs": [
                "",
                "https://jalammar.github.io/images/t/output_target_probability_distributions.png",
                "https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png",
                "https://jalammar.github.io/images/t/The_transformer_encoders_decoders.png",
                "https://jalammar.github.io/images/t/transformer_attention_heads_weight_matrix_o.png",
                "https://jalammar.github.io/images/t/self-attention-matrix-calculation.png",
                "https://jalammar.github.io/images/t/transformer_self-attention_visualization_3.png",
                "https://jalammar.github.io/images/t/transformer_positional_encoding_large_example.png",
                "https://miro.medium.com/v2/resize:fill:144:144/2*DW37pyGS3g1xXe5_wOB1nw.png",
                "https://jalammar.github.io/images/t/encoder_with_tensors_2.png",
                "https://jalammar.github.io/images/t/Transformer_encoder.png",
                "https://jalammar.github.io/images/t/transformer_attention_heads_qkv.png",
                "https://jalammar.github.io/images/t/transformer_self-attention_visualization.png",
                "https://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png",
                "https://jalammar.github.io/images/t/self-attention-output.png",
                "https://jalammar.github.io/images/t/encoder_with_tensors.png",
                "https://jalammar.github.io/images/t/transformer_self_attention_vectors.png",
                "https://miro.medium.com/v2/resize:fill:64:64/1*CJe3891yB1A1mzMdqemkdg.jpeg",
                "https://jalammar.github.io/images/t/transformer_self-attention_visualization_2.png",
                "https://jalammar.github.io/images/t/transformer_positional_encoding_example.png",
                "https://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png",
                "https://jalammar.github.io/images/t/transformer_positional_encoding_vectors.png",
                "https://jalammar.github.io/images/t/transformer_decoding_2.gif",
                "https://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png",
                "https://jalammar.github.io/images/t/embeddings.png",
                "https://jalammar.github.io/images/t/output_trained_model_probability_distributions.png",
                "https://jalammar.github.io/images/t/attention-is-all-you-need-positional-encoding.png",
                "https://jalammar.github.io/images/t/transformer_logits_output_and_label.png",
                "https://jalammar.github.io/images/t/transformer_decoder_output_softmax.png",
                "https://jalammar.github.io/images/t/the_transformer_3.png",
                "https://jalammar.github.io/images/t/vocabulary.png",
                "https://jalammar.github.io/images/t/Transformer_decoder.png",
                "https://jalammar.github.io/images/t/transformer_attention_heads_z.png",
                "https://jalammar.github.io/images/t/transformer_self_attention_score.png",
                "https://jalammar.github.io/images/t/transformer_resideual_layer_norm_2.png",
                "https://jalammar.github.io/images/t/one-hot-vocabulary-example.png",
                "https://jalammar.github.io/images/t/self-attention_softmax.png",
                "https://jalammar.github.io/images/t/transformer_decoding_1.gif",
                "https://jalammar.github.io/images/t/The_transformer_encoder_decoder_stack.png"
            ]
        }
    },
    {
        "heading": "The Transformer",
        "content": "### Training\n- Use one-hot encoding to represent words in output vocabulary.\n- Compare model output to target distribution using cross-entropy or Kullback\u2013Leibler divergence.\n- Use beam search to generate multiple translations.\n\n",
        "metadata": {
            "external_references": {
                "": "",
                "Press": "pressinquiries@medium.com?source=post_page-----4ac19b67fb4d--------------------------------"
            },
            "sources": [
                "https://jalammar.github.io/illustrated-transformer/",
                "https://towardsdatascience.com/test-time-augmentation-tta-and-how-to-perform-it-with-keras-4ac19b67fb4d"
            ],
            "imgs": [
                "",
                "https://jalammar.github.io/images/t/output_target_probability_distributions.png",
                "https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png",
                "https://jalammar.github.io/images/t/The_transformer_encoders_decoders.png",
                "https://jalammar.github.io/images/t/transformer_attention_heads_weight_matrix_o.png",
                "https://jalammar.github.io/images/t/self-attention-matrix-calculation.png",
                "https://jalammar.github.io/images/t/transformer_self-attention_visualization_3.png",
                "https://jalammar.github.io/images/t/transformer_positional_encoding_large_example.png",
                "https://miro.medium.com/v2/resize:fill:144:144/2*DW37pyGS3g1xXe5_wOB1nw.png",
                "https://jalammar.github.io/images/t/encoder_with_tensors_2.png",
                "https://jalammar.github.io/images/t/Transformer_encoder.png",
                "https://jalammar.github.io/images/t/transformer_attention_heads_qkv.png",
                "https://jalammar.github.io/images/t/transformer_self-attention_visualization.png",
                "https://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png",
                "https://jalammar.github.io/images/t/self-attention-output.png",
                "https://jalammar.github.io/images/t/encoder_with_tensors.png",
                "https://jalammar.github.io/images/t/transformer_self_attention_vectors.png",
                "https://miro.medium.com/v2/resize:fill:64:64/1*CJe3891yB1A1mzMdqemkdg.jpeg",
                "https://jalammar.github.io/images/t/transformer_self-attention_visualization_2.png",
                "https://jalammar.github.io/images/t/transformer_positional_encoding_example.png",
                "https://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png",
                "https://jalammar.github.io/images/t/transformer_positional_encoding_vectors.png",
                "https://jalammar.github.io/images/t/transformer_decoding_2.gif",
                "https://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png",
                "https://jalammar.github.io/images/t/embeddings.png",
                "https://jalammar.github.io/images/t/output_trained_model_probability_distributions.png",
                "https://jalammar.github.io/images/t/attention-is-all-you-need-positional-encoding.png",
                "https://jalammar.github.io/images/t/transformer_logits_output_and_label.png",
                "https://jalammar.github.io/images/t/transformer_decoder_output_softmax.png",
                "https://jalammar.github.io/images/t/the_transformer_3.png",
                "https://jalammar.github.io/images/t/vocabulary.png",
                "https://jalammar.github.io/images/t/Transformer_decoder.png",
                "https://jalammar.github.io/images/t/transformer_attention_heads_z.png",
                "https://jalammar.github.io/images/t/transformer_self_attention_score.png",
                "https://jalammar.github.io/images/t/transformer_resideual_layer_norm_2.png",
                "https://jalammar.github.io/images/t/one-hot-vocabulary-example.png",
                "https://jalammar.github.io/images/t/self-attention_softmax.png",
                "https://jalammar.github.io/images/t/transformer_decoding_1.gif",
                "https://jalammar.github.io/images/t/The_transformer_encoder_decoder_stack.png"
            ]
        }
    },
    {
        "heading": "The Transformer",
        "content": "### Additional Resources\n- [Attention Is All You Need paper](https://arxiv.org/abs/1706.03762)\n- [Transformer blog post](https://blog.tensorflow.org/2017/08/transformer-novel-neural-network.html)\n- [Tensor2Tensor announcement](https://research.googleblog.com/2017/08/tensor2tensor-state-of-art-neural.html)\n- [\u0141ukasz Kaiser's talk](https://www.youtube.com/watch?v=4-h59s4_03Y)\n- [Tensor2Tensor Jupyter Notebook](https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/transformer_chatbot.ipynb)\n- [Tensor2Tensor repo](https://github.com/tensorflow/tensor2tensor)",
        "metadata": {
            "external_references": {
                "": "",
                "Press": "pressinquiries@medium.com?source=post_page-----4ac19b67fb4d--------------------------------"
            },
            "sources": [
                "https://jalammar.github.io/illustrated-transformer/",
                "https://towardsdatascience.com/test-time-augmentation-tta-and-how-to-perform-it-with-keras-4ac19b67fb4d"
            ],
            "imgs": [
                "",
                "https://jalammar.github.io/images/t/output_target_probability_distributions.png",
                "https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png",
                "https://jalammar.github.io/images/t/The_transformer_encoders_decoders.png",
                "https://jalammar.github.io/images/t/transformer_attention_heads_weight_matrix_o.png",
                "https://jalammar.github.io/images/t/self-attention-matrix-calculation.png",
                "https://jalammar.github.io/images/t/transformer_self-attention_visualization_3.png",
                "https://jalammar.github.io/images/t/transformer_positional_encoding_large_example.png",
                "https://miro.medium.com/v2/resize:fill:144:144/2*DW37pyGS3g1xXe5_wOB1nw.png",
                "https://jalammar.github.io/images/t/encoder_with_tensors_2.png",
                "https://jalammar.github.io/images/t/Transformer_encoder.png",
                "https://jalammar.github.io/images/t/transformer_attention_heads_qkv.png",
                "https://jalammar.github.io/images/t/transformer_self-attention_visualization.png",
                "https://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png",
                "https://jalammar.github.io/images/t/self-attention-output.png",
                "https://jalammar.github.io/images/t/encoder_with_tensors.png",
                "https://jalammar.github.io/images/t/transformer_self_attention_vectors.png",
                "https://miro.medium.com/v2/resize:fill:64:64/1*CJe3891yB1A1mzMdqemkdg.jpeg",
                "https://jalammar.github.io/images/t/transformer_self-attention_visualization_2.png",
                "https://jalammar.github.io/images/t/transformer_positional_encoding_example.png",
                "https://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png",
                "https://jalammar.github.io/images/t/transformer_positional_encoding_vectors.png",
                "https://jalammar.github.io/images/t/transformer_decoding_2.gif",
                "https://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png",
                "https://jalammar.github.io/images/t/embeddings.png",
                "https://jalammar.github.io/images/t/output_trained_model_probability_distributions.png",
                "https://jalammar.github.io/images/t/attention-is-all-you-need-positional-encoding.png",
                "https://jalammar.github.io/images/t/transformer_logits_output_and_label.png",
                "https://jalammar.github.io/images/t/transformer_decoder_output_softmax.png",
                "https://jalammar.github.io/images/t/the_transformer_3.png",
                "https://jalammar.github.io/images/t/vocabulary.png",
                "https://jalammar.github.io/images/t/Transformer_decoder.png",
                "https://jalammar.github.io/images/t/transformer_attention_heads_z.png",
                "https://jalammar.github.io/images/t/transformer_self_attention_score.png",
                "https://jalammar.github.io/images/t/transformer_resideual_layer_norm_2.png",
                "https://jalammar.github.io/images/t/one-hot-vocabulary-example.png",
                "https://jalammar.github.io/images/t/self-attention_softmax.png",
                "https://jalammar.github.io/images/t/transformer_decoding_1.gif",
                "https://jalammar.github.io/images/t/The_transformer_encoder_decoder_stack.png"
            ]
        }
    },
    {
        "heading": "Data Augmentation",
        "content": "- Process of randomly applying operations (rotation, zoom, shift, flips, etc.) to input data.\n- Prevents the model from seeing the exact same example twice.\n- Forces the model to learn more general features about the classes it needs to recognize.",
        "metadata": {
            "external_references": {
                "": ""
            },
            "sources": [
                "https://towardsdatascience.com/test-time-augmentation-tta-and-how-to-perform-it-with-keras-4ac19b67fb4d"
            ],
            "imgs": [
                ""
            ]
        }
    },
    {
        "heading": "Test Time Augmentation",
        "content": "- Similar to Data Augmentation, but applied to test images.\n- Randomly modifies test images.\n- Shows augmented images to the trained model multiple times.\n- Averages the predictions for each corresponding image to obtain the final guess.\n\n",
        "metadata": {
            "external_references": {
                "": ""
            },
            "sources": [
                "https://towardsdatascience.com/test-time-augmentation-tta-and-how-to-perform-it-with-keras-4ac19b67fb4d"
            ],
            "imgs": [
                ""
            ]
        }
    },
    {
        "heading": "Test Time Augmentation",
        "content": "### Benefits of Test Time Augmentation\n\n- Particularly useful for test images that the model is unsure about.\n- Can improve accuracy for images that are different from the training data.\n\n",
        "metadata": {
            "external_references": {
                "": ""
            },
            "sources": [
                "https://towardsdatascience.com/test-time-augmentation-tta-and-how-to-perform-it-with-keras-4ac19b67fb4d"
            ],
            "imgs": [
                ""
            ]
        }
    },
    {
        "heading": "Test Time Augmentation",
        "content": "### Implementation\n\n- Can reuse the same Data Generator used for training.\n- Apply the Data Generator to validation images.\n\n",
        "metadata": {
            "external_references": {
                "": ""
            },
            "sources": [
                "https://towardsdatascience.com/test-time-augmentation-tta-and-how-to-perform-it-with-keras-4ac19b67fb4d"
            ],
            "imgs": [
                ""
            ]
        }
    },
    {
        "heading": "Test Time Augmentation",
        "content": "### Example\n\n- Present 5 slightly modified versions of the same image to the network.\n- Network predicts the class of each image.\n- Predictions may vary significantly, indicating that the model is unsure about the image.",
        "metadata": {
            "external_references": {
                "": ""
            },
            "sources": [
                "https://towardsdatascience.com/test-time-augmentation-tta-and-how-to-perform-it-with-keras-4ac19b67fb4d"
            ],
            "imgs": [
                ""
            ]
        }
    }
]