const BLOG_AI = [
  {
    text: '## Data Augmentation and Test Time Augmentation\n\nThis article discusses the benefits of data augmentation and test time augmentation (TTA) for improving the performance of neural networks.\n\n### Data Augmentation\n\n* *Definition:* Data augmentation is a technique used to virtually increase the size of a training dataset by creating modified versions of existing data.\n* *Benefits:\n    * Improves model robustness to slight changes in input data.\n    * Reduces overfitting.\n *Examples:\n    * **Image data:* Flipping, rotating, cropping, adding noise.\n    * *Text data:* Synonym replacement, adding random words.\n\n### Test Time Augmentation\n\n* *Definition:* TTA is a technique used to improve the performance of a model during testing by applying data augmentation to the test data.\n* *Benefits:\n    * Enhances model accuracy by averaging predictions from multiple augmented versions of the test data.\n *Example:\n    * Applying horizontal flips to test images and averaging the predictions from the original and flipped images.\n\n### Importance of Choosing the Right Augmentation Techniques\n\n The type of augmentation used should be tailored to the specific dataset and task.\n* *MNIST:* Random flips or large rotations can be detrimental as they change the content of the image.\n* *CIFAR10:* Horizontal flips are beneficial as they don\'t change the image content.\n* *Satellite imaging or crop cultures images:* Large rotations and vertical flips can be used as they don\'t affect the meaning of the images.\n\n### Implementing TTA with Keras\n\n* TTA can be easily implemented in Keras, although it is not explicitly mentioned in the documentation.\n* The article provides a simple convolutional neural network example trained on CIFAR10 to demonstrate TTA implementation.\n\n## The Transformer: A Deep Dive into Attention\n\nThis article provides a detailed explanation of the Transformer model, a powerful architecture that utilizes attention mechanisms for efficient and accurate machine translation.\n\n### Overview\n\n* *Purpose:* The Transformer model is designed to improve the speed and performance of neural machine translation.\n* *Key Features:\n    * **Attention:* Allows the model to focus on relevant parts of the input sentence.\n    * *Parallelization:* Enables efficient training by allowing parallel processing of different parts of the input sequence.\n* *Advantages:\n    * Outperforms traditional sequence-to-sequence models in specific tasks.\n    * Highly parallelizable, making it suitable for large-scale training.\n\n### Model Architecture\n\n *Encoding Component:\n    * Stack of encoders (typically 6).\n    * Each encoder consists of:\n        * **Self-attention layer:* Allows the model to attend to different parts of the input sequence.\n        * *Feed-forward layer:* Applies a non-linear transformation to the input.\n* *Decoding Component:\n    * Stack of decoders (same number as encoders).\n    * Each decoder consists of:\n        * **Masked self-attention layer:* Prevents the decoder from attending to future words in the output sequence.\n        * *Encoder-decoder attention layer:* Allows the decoder to attend to relevant parts of the encoded input.\n        * *Feed-forward layer:* Applies a non-linear transformation to the input.\n\n### Attention Mechanism\n\n* *Self-attention:* Allows the model to understand the relationships between words in a sentence.\n* *Multi-headed attention:* Enhances the model\'s ability to focus on different positions and representation subspaces.\n* *Positional encoding:* Provides information about the position of each word in the sequence.\n\n### Training the Transformer\n\n* *Output vocabulary:* The model learns a set of unique words that it can output.\n* *One-hot encoding:* Each word in the vocabulary is represented by a vector with a single "1" and all other values set to "0".\n* *Cross-entropy loss:* Measures the difference between the predicted probability distribution and the actual probability distribution of the target word.\n* *Beam search:* A decoding algorithm that explores multiple possible translations and selects the best one.\n\n### Conclusion\n\nThe Transformer model is a powerful and efficient architecture that has revolutionized machine translation. Its use of attention mechanisms and parallelization capabilities make it a highly effective tool for natural language processing tasks.\n\n### Further Resources\n\n* *Attention Is All You Need paper:* Provides a detailed explanation of the Transformer model.\n* *Transformer blog post:* Offers a high-level overview of the Transformer architecture.\n* *Tensor2Tensor announcement:* Introduces the Tensor2Tensor library, which includes a TensorFlow implementation of the Transformer.\n* *Łukasz Kaiser’s talk:* Provides a walkthrough of the Transformer model and its details.\n* *Jupyter Notebook:* Allows users to interact with a trained Transformer model.\n\n### Related Work\n\n* *Depthwise Separable Convolutions for Neural Machine Translation\n *One Model To Learn Them All\n *Discrete Autoencoders for Sequence Models\n *Generating Wikipedia by Summarizing Long Sequences\n *Image Transformer\n *Training Tips for the Transformer Model\n *Self-Attention with Relative Position Representations\n *Fast Decoding in Sequence Models using Discrete Latent Variables\n *Adafactor: Adaptive Learning Rates with Sublinear Memory Cost*',
    external_references: {
      "": "",
      Press: "pressinquiries@medium.com?source=post_page-----4ac19b67fb4d--------------------------------",
    },
    sources: [
      "https://towardsdatascience.com/test-time-augmentation-tta-and-how-to-perform-it-with-keras-4ac19b67fb4d",
      "https://jalammar.github.io/illustrated-transformer/",
    ],
    imgs: [
      "",
      "https://jalammar.github.io/images/t/self-attention-output.png",
      "https://jalammar.github.io/images/t/transformer_positional_encoding_large_example.png",
      "https://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png",
      "https://miro.medium.com/v2/resize:fill:64:64/1*CJe3891yB1A1mzMdqemkdg.jpeg",
      "https://jalammar.github.io/images/t/The_transformer_encoder_decoder_stack.png",
      "https://jalammar.github.io/images/t/transformer_attention_heads_weight_matrix_o.png",
      "https://jalammar.github.io/images/t/transformer_self-attention_visualization_3.png",
      "https://jalammar.github.io/images/t/transformer_self-attention_visualization_2.png",
      "https://jalammar.github.io/images/t/encoder_with_tensors.png",
      "https://jalammar.github.io/images/t/encoder_with_tensors_2.png",
      "https://miro.medium.com/v2/resize:fill:144:144/2*DW37pyGS3g1xXe5_wOB1nw.png",
      "https://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png",
      "https://jalammar.github.io/images/t/transformer_self_attention_score.png",
      "https://jalammar.github.io/images/t/transformer_resideual_layer_norm_2.png",
      "https://jalammar.github.io/images/t/one-hot-vocabulary-example.png",
      "https://jalammar.github.io/images/t/self-attention-matrix-calculation.png",
      "https://jalammar.github.io/images/t/transformer_positional_encoding_vectors.png",
      "https://jalammar.github.io/images/t/transformer_logits_output_and_label.png",
      "https://jalammar.github.io/images/t/transformer_positional_encoding_example.png",
      "https://jalammar.github.io/images/t/Transformer_encoder.png",
      "https://jalammar.github.io/images/t/transformer_decoding_1.gif",
      "https://jalammar.github.io/images/t/transformer_self_attention_vectors.png",
      "https://jalammar.github.io/images/t/transformer_decoder_output_softmax.png",
      "https://jalammar.github.io/images/t/self-attention_softmax.png",
      "https://jalammar.github.io/images/t/Transformer_decoder.png",
      "https://jalammar.github.io/images/t/The_transformer_encoders_decoders.png",
      "https://jalammar.github.io/images/t/vocabulary.png",
      "https://jalammar.github.io/images/t/transformer_self-attention_visualization.png",
      "https://jalammar.github.io/images/t/output_target_probability_distributions.png",
      "https://jalammar.github.io/images/t/attention-is-all-you-need-positional-encoding.png",
      "https://jalammar.github.io/images/t/transformer_attention_heads_qkv.png",
      "https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png",
      "https://jalammar.github.io/images/t/the_transformer_3.png",
      "https://jalammar.github.io/images/t/output_trained_model_probability_distributions.png",
      "https://jalammar.github.io/images/t/transformer_attention_heads_z.png",
      "https://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png",
      "https://jalammar.github.io/images/t/transformer_decoding_2.gif",
      "https://jalammar.github.io/images/t/embeddings.png",
    ],
  },
  {
    text: '## Data Augmentation\n\nData Augmentation is the process of randomly applying operations (rotation, zoom, shift, flips,…) to the input data. This ensures that the model is never shown the exact same example twice, forcing it to learn more general features about the classes it needs to recognize.\n\n## Test Time Augmentation\n\nSimilar to Data Augmentation on the training set, Test Time Augmentation performs random modifications to the test images. Instead of showing the "clean" images only once to the trained model, we show it the augmented images several times. The predictions for each corresponding image are then averaged to arrive at the final guess.\n\n### Example of Test Time Augmentation\n\nImagine presenting 5 slightly modified versions of the same image to a network and asking it to predict the class of each. The corresponding predictions might look like this:\n\n* *Image 1:* Prediction A\n* *Image 2:* Prediction B\n* *Image 3:* Prediction A\n* *Image 4:* Prediction A\n* *Image 5:* Prediction B\n\n### Benefits of Test Time Augmentation\n\nTest Time Augmentation is particularly useful for test images that the model is unsure about. Even if those 5 images seem very similar to you, the model might perceive them as very different, as evidenced by its predictions.\n\n### Implementing Test Time Augmentation\n\nTo implement Test Time Augmentation, you can reuse the same Data Generator used for training and apply it to validation images.\n\n### Training with Test Time Augmentation\n\nAfter applying Test Time Augmentation, you can train the network for a few epochs. The final accuracy of the model on the validation images can then be evaluated.',
    external_references: {
      "external references":
        "https://towardsdatascience.com/test-time-augmentation-tta-and-how-to-perform-it-with-keras-4ac19b67fb4d",
    },
    sources: [
      "https://towardsdatascience.com/test-time-augmentation-tta-and-how-to-perform-it-with-keras-4ac19b67fb4d",
    ],
    imgs: ["https://jalammar.github.io/images/t/self-attention-output.png"],
  },
];

interface watch_ai {
  heading: string;
  content: string;
  metadata: {
    external_references: {};
    sources: string[];
    imgs: string[];
  };
}

const WATCH_AI: watch_ai[] = [
  {
    heading: "Data Augmentation and Test Time Augmentation",
    content:
      "This article discusses the benefits of data augmentation and test time augmentation (TTA) for improving the performance of neural networks.\n\n",
    metadata: {
      external_references: {
        "": "",
        Press: "pressinquiries@medium.com?source=post_page-----4ac19b67fb4d--------------------------------",
      },
      sources: [
        "https://towardsdatascience.com/test-time-augmentation-tta-and-how-to-perform-it-with-keras-4ac19b67fb4d",
        "https://jalammar.github.io/illustrated-transformer/",
      ],
      imgs: [
        "https://jalammar.github.io/images/t/self-attention-output.png",
        "https://jalammar.github.io/images/t/transformer_positional_encoding_large_example.png",
        "https://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png",
        "https://miro.medium.com/v2/resize:fill:64:64/1*CJe3891yB1A1mzMdqemkdg.jpeg",
        "https://jalammar.github.io/images/t/The_transformer_encoder_decoder_stack.png",
        "https://jalammar.github.io/images/t/transformer_attention_heads_weight_matrix_o.png",
        "https://jalammar.github.io/images/t/transformer_self-attention_visualization_3.png",
        "https://jalammar.github.io/images/t/transformer_self-attention_visualization_2.png",
        "https://jalammar.github.io/images/t/encoder_with_tensors.png",
        "https://jalammar.github.io/images/t/encoder_with_tensors_2.png",
        "https://miro.medium.com/v2/resize:fill:144:144/2*DW37pyGS3g1xXe5_wOB1nw.png",
        "https://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png",
        "https://jalammar.github.io/images/t/transformer_self_attention_score.png",
        "https://jalammar.github.io/images/t/transformer_resideual_layer_norm_2.png",
        "https://jalammar.github.io/images/t/one-hot-vocabulary-example.png",
        "https://jalammar.github.io/images/t/self-attention-matrix-calculation.png",
        "https://jalammar.github.io/images/t/transformer_positional_encoding_vectors.png",
        "https://jalammar.github.io/images/t/transformer_logits_output_and_label.png",
        "https://jalammar.github.io/images/t/transformer_positional_encoding_example.png",
        "https://jalammar.github.io/images/t/Transformer_encoder.png",
        "https://jalammar.github.io/images/t/transformer_decoding_1.gif",
        "https://jalammar.github.io/images/t/transformer_self_attention_vectors.png",
        "https://jalammar.github.io/images/t/transformer_decoder_output_softmax.png",
        "https://jalammar.github.io/images/t/self-attention_softmax.png",
        "https://jalammar.github.io/images/t/Transformer_decoder.png",
        "https://jalammar.github.io/images/t/The_transformer_encoders_decoders.png",
        "https://jalammar.github.io/images/t/vocabulary.png",
        "https://jalammar.github.io/images/t/transformer_self-attention_visualization.png",
        "https://jalammar.github.io/images/t/output_target_probability_distributions.png",
        "https://jalammar.github.io/images/t/attention-is-all-you-need-positional-encoding.png",
        "https://jalammar.github.io/images/t/transformer_attention_heads_qkv.png",
        "https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png",
        "https://jalammar.github.io/images/t/the_transformer_3.png",
        "https://jalammar.github.io/images/t/output_trained_model_probability_distributions.png",
        "https://jalammar.github.io/images/t/transformer_attention_heads_z.png",
        "https://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png",
        "https://jalammar.github.io/images/t/transformer_decoding_2.gif",
        "https://jalammar.github.io/images/t/embeddings.png",
      ],
    },
  },
  {
    heading: "Data Augmentation and Test Time Augmentation",
    content:
      "### Data Augmentation\n\n* *Definition:* Data augmentation is a technique used to virtually increase the size of a training dataset by creating modified versions of existing data.\n* *Benefits:\n    * Improves model robustness to slight changes in input data.\n    * Reduces overfitting.\n *Examples:\n    * **Image data:* Flipping, rotating, cropping, adding noise.\n    * *Text data:* Synonym replacement, adding random words.\n\n",
    metadata: {
      external_references: {
        "": "",
        Press: "pressinquiries@medium.com?source=post_page-----4ac19b67fb4d--------------------------------",
      },
      sources: [
        "https://towardsdatascience.com/test-time-augmentation-tta-and-how-to-perform-it-with-keras-4ac19b67fb4d",
        "https://jalammar.github.io/illustrated-transformer/",
      ],
      imgs: [
        "",
        "https://jalammar.github.io/images/t/self-attention-output.png",
        "https://jalammar.github.io/images/t/transformer_positional_encoding_large_example.png",
        "https://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png",
        "https://miro.medium.com/v2/resize:fill:64:64/1*CJe3891yB1A1mzMdqemkdg.jpeg",
        "https://jalammar.github.io/images/t/The_transformer_encoder_decoder_stack.png",
        "https://jalammar.github.io/images/t/transformer_attention_heads_weight_matrix_o.png",
        "https://jalammar.github.io/images/t/transformer_self-attention_visualization_3.png",
        "https://jalammar.github.io/images/t/transformer_self-attention_visualization_2.png",
        "https://jalammar.github.io/images/t/encoder_with_tensors.png",
        "https://jalammar.github.io/images/t/encoder_with_tensors_2.png",
        "https://miro.medium.com/v2/resize:fill:144:144/2*DW37pyGS3g1xXe5_wOB1nw.png",
        "https://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png",
        "https://jalammar.github.io/images/t/transformer_self_attention_score.png",
        "https://jalammar.github.io/images/t/transformer_resideual_layer_norm_2.png",
        "https://jalammar.github.io/images/t/one-hot-vocabulary-example.png",
        "https://jalammar.github.io/images/t/self-attention-matrix-calculation.png",
        "https://jalammar.github.io/images/t/transformer_positional_encoding_vectors.png",
        "https://jalammar.github.io/images/t/transformer_logits_output_and_label.png",
        "https://jalammar.github.io/images/t/transformer_positional_encoding_example.png",
        "https://jalammar.github.io/images/t/Transformer_encoder.png",
        "https://jalammar.github.io/images/t/transformer_decoding_1.gif",
        "https://jalammar.github.io/images/t/transformer_self_attention_vectors.png",
        "https://jalammar.github.io/images/t/transformer_decoder_output_softmax.png",
        "https://jalammar.github.io/images/t/self-attention_softmax.png",
        "https://jalammar.github.io/images/t/Transformer_decoder.png",
        "https://jalammar.github.io/images/t/The_transformer_encoders_decoders.png",
        "https://jalammar.github.io/images/t/vocabulary.png",
        "https://jalammar.github.io/images/t/transformer_self-attention_visualization.png",
        "https://jalammar.github.io/images/t/output_target_probability_distributions.png",
        "https://jalammar.github.io/images/t/attention-is-all-you-need-positional-encoding.png",
        "https://jalammar.github.io/images/t/transformer_attention_heads_qkv.png",
        "https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png",
        "https://jalammar.github.io/images/t/the_transformer_3.png",
        "https://jalammar.github.io/images/t/output_trained_model_probability_distributions.png",
        "https://jalammar.github.io/images/t/transformer_attention_heads_z.png",
        "https://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png",
        "https://jalammar.github.io/images/t/transformer_decoding_2.gif",
        "https://jalammar.github.io/images/t/embeddings.png",
      ],
    },
  },
  {
    heading: "Data Augmentation and Test Time Augmentation",
    content:
      "### Test Time Augmentation\n\n* *Definition:* TTA is a technique used to improve the performance of a model during testing by applying data augmentation to the test data.\n* *Benefits:\n    * Enhances model accuracy by averaging predictions from multiple augmented versions of the test data.\n *Example:*\n    * Applying horizontal flips to test images and averaging the predictions from the original and flipped images.\n\n",
    metadata: {
      external_references: {
        "": "",
        Press: "pressinquiries@medium.com?source=post_page-----4ac19b67fb4d--------------------------------",
      },
      sources: [
        "https://towardsdatascience.com/test-time-augmentation-tta-and-how-to-perform-it-with-keras-4ac19b67fb4d",
        "https://jalammar.github.io/illustrated-transformer/",
      ],
      imgs: [
        "",
        "https://jalammar.github.io/images/t/self-attention-output.png",
        "https://jalammar.github.io/images/t/transformer_positional_encoding_large_example.png",
        "https://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png",
        "https://miro.medium.com/v2/resize:fill:64:64/1*CJe3891yB1A1mzMdqemkdg.jpeg",
        "https://jalammar.github.io/images/t/The_transformer_encoder_decoder_stack.png",
        "https://jalammar.github.io/images/t/transformer_attention_heads_weight_matrix_o.png",
        "https://jalammar.github.io/images/t/transformer_self-attention_visualization_3.png",
        "https://jalammar.github.io/images/t/transformer_self-attention_visualization_2.png",
        "https://jalammar.github.io/images/t/encoder_with_tensors.png",
        "https://jalammar.github.io/images/t/encoder_with_tensors_2.png",
        "https://miro.medium.com/v2/resize:fill:144:144/2*DW37pyGS3g1xXe5_wOB1nw.png",
        "https://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png",
        "https://jalammar.github.io/images/t/transformer_self_attention_score.png",
        "https://jalammar.github.io/images/t/transformer_resideual_layer_norm_2.png",
        "https://jalammar.github.io/images/t/one-hot-vocabulary-example.png",
        "https://jalammar.github.io/images/t/self-attention-matrix-calculation.png",
        "https://jalammar.github.io/images/t/transformer_positional_encoding_vectors.png",
        "https://jalammar.github.io/images/t/transformer_logits_output_and_label.png",
        "https://jalammar.github.io/images/t/transformer_positional_encoding_example.png",
        "https://jalammar.github.io/images/t/Transformer_encoder.png",
        "https://jalammar.github.io/images/t/transformer_decoding_1.gif",
        "https://jalammar.github.io/images/t/transformer_self_attention_vectors.png",
        "https://jalammar.github.io/images/t/transformer_decoder_output_softmax.png",
        "https://jalammar.github.io/images/t/self-attention_softmax.png",
        "https://jalammar.github.io/images/t/Transformer_decoder.png",
        "https://jalammar.github.io/images/t/The_transformer_encoders_decoders.png",
        "https://jalammar.github.io/images/t/vocabulary.png",
        "https://jalammar.github.io/images/t/transformer_self-attention_visualization.png",
        "https://jalammar.github.io/images/t/output_target_probability_distributions.png",
        "https://jalammar.github.io/images/t/attention-is-all-you-need-positional-encoding.png",
        "https://jalammar.github.io/images/t/transformer_attention_heads_qkv.png",
        "https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png",
        "https://jalammar.github.io/images/t/the_transformer_3.png",
        "https://jalammar.github.io/images/t/output_trained_model_probability_distributions.png",
        "https://jalammar.github.io/images/t/transformer_attention_heads_z.png",
        "https://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png",
        "https://jalammar.github.io/images/t/transformer_decoding_2.gif",
        "https://jalammar.github.io/images/t/embeddings.png",
      ],
    },
  },
  {
    heading: "Data Augmentation and Test Time Augmentation",
    content:
      "### Importance of Choosing the Right Augmentation Techniques\n\n* The type of augmentation used should be tailored to the specific dataset and task.\n* *MNIST:* Random flips or large rotations can be detrimental as they change the content of the image.\n* *CIFAR10:* Horizontal flips are beneficial as they don't change the image content.\n* *Satellite imaging or crop cultures images:* Large rotations and vertical flips can be used as they don't affect the meaning of the images.\n\n",
    metadata: {
      external_references: {
        "": "",
        Press: "pressinquiries@medium.com?source=post_page-----4ac19b67fb4d--------------------------------",
      },
      sources: [
        "https://towardsdatascience.com/test-time-augmentation-tta-and-how-to-perform-it-with-keras-4ac19b67fb4d",
        "https://jalammar.github.io/illustrated-transformer/",
      ],
      imgs: [
        "",
        "https://jalammar.github.io/images/t/self-attention-output.png",
        "https://jalammar.github.io/images/t/transformer_positional_encoding_large_example.png",
        "https://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png",
        "https://miro.medium.com/v2/resize:fill:64:64/1*CJe3891yB1A1mzMdqemkdg.jpeg",
        "https://jalammar.github.io/images/t/The_transformer_encoder_decoder_stack.png",
        "https://jalammar.github.io/images/t/transformer_attention_heads_weight_matrix_o.png",
        "https://jalammar.github.io/images/t/transformer_self-attention_visualization_3.png",
        "https://jalammar.github.io/images/t/transformer_self-attention_visualization_2.png",
        "https://jalammar.github.io/images/t/encoder_with_tensors.png",
        "https://jalammar.github.io/images/t/encoder_with_tensors_2.png",
        "https://miro.medium.com/v2/resize:fill:144:144/2*DW37pyGS3g1xXe5_wOB1nw.png",
        "https://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png",
        "https://jalammar.github.io/images/t/transformer_self_attention_score.png",
        "https://jalammar.github.io/images/t/transformer_resideual_layer_norm_2.png",
        "https://jalammar.github.io/images/t/one-hot-vocabulary-example.png",
        "https://jalammar.github.io/images/t/self-attention-matrix-calculation.png",
        "https://jalammar.github.io/images/t/transformer_positional_encoding_vectors.png",
        "https://jalammar.github.io/images/t/transformer_logits_output_and_label.png",
        "https://jalammar.github.io/images/t/transformer_positional_encoding_example.png",
        "https://jalammar.github.io/images/t/Transformer_encoder.png",
        "https://jalammar.github.io/images/t/transformer_decoding_1.gif",
        "https://jalammar.github.io/images/t/transformer_self_attention_vectors.png",
        "https://jalammar.github.io/images/t/transformer_decoder_output_softmax.png",
        "https://jalammar.github.io/images/t/self-attention_softmax.png",
        "https://jalammar.github.io/images/t/Transformer_decoder.png",
        "https://jalammar.github.io/images/t/The_transformer_encoders_decoders.png",
        "https://jalammar.github.io/images/t/vocabulary.png",
        "https://jalammar.github.io/images/t/transformer_self-attention_visualization.png",
        "https://jalammar.github.io/images/t/output_target_probability_distributions.png",
        "https://jalammar.github.io/images/t/attention-is-all-you-need-positional-encoding.png",
        "https://jalammar.github.io/images/t/transformer_attention_heads_qkv.png",
        "https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png",
        "https://jalammar.github.io/images/t/the_transformer_3.png",
        "https://jalammar.github.io/images/t/output_trained_model_probability_distributions.png",
        "https://jalammar.github.io/images/t/transformer_attention_heads_z.png",
        "https://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png",
        "https://jalammar.github.io/images/t/transformer_decoding_2.gif",
        "https://jalammar.github.io/images/t/embeddings.png",
      ],
    },
  },
  {
    heading: "Data Augmentation and Test Time Augmentation",
    content:
      "### Implementing TTA with Keras\n\n* TTA can be easily implemented in Keras, although it is not explicitly mentioned in the documentation.\n* The article provides a simple convolutional neural network example trained on CIFAR10 to demonstrate TTA implementation.",
    metadata: {
      external_references: {
        "": "",
        Press: "pressinquiries@medium.com?source=post_page-----4ac19b67fb4d--------------------------------",
      },
      sources: [
        "https://towardsdatascience.com/test-time-augmentation-tta-and-how-to-perform-it-with-keras-4ac19b67fb4d",
        "https://jalammar.github.io/illustrated-transformer/",
      ],
      imgs: [
        "",
        "https://jalammar.github.io/images/t/self-attention-output.png",
        "https://jalammar.github.io/images/t/transformer_positional_encoding_large_example.png",
        "https://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png",
        "https://miro.medium.com/v2/resize:fill:64:64/1*CJe3891yB1A1mzMdqemkdg.jpeg",
        "https://jalammar.github.io/images/t/The_transformer_encoder_decoder_stack.png",
        "https://jalammar.github.io/images/t/transformer_attention_heads_weight_matrix_o.png",
        "https://jalammar.github.io/images/t/transformer_self-attention_visualization_3.png",
        "https://jalammar.github.io/images/t/transformer_self-attention_visualization_2.png",
        "https://jalammar.github.io/images/t/encoder_with_tensors.png",
        "https://jalammar.github.io/images/t/encoder_with_tensors_2.png",
        "https://miro.medium.com/v2/resize:fill:144:144/2*DW37pyGS3g1xXe5_wOB1nw.png",
        "https://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png",
        "https://jalammar.github.io/images/t/transformer_self_attention_score.png",
        "https://jalammar.github.io/images/t/transformer_resideual_layer_norm_2.png",
        "https://jalammar.github.io/images/t/one-hot-vocabulary-example.png",
        "https://jalammar.github.io/images/t/self-attention-matrix-calculation.png",
        "https://jalammar.github.io/images/t/transformer_positional_encoding_vectors.png",
        "https://jalammar.github.io/images/t/transformer_logits_output_and_label.png",
        "https://jalammar.github.io/images/t/transformer_positional_encoding_example.png",
        "https://jalammar.github.io/images/t/Transformer_encoder.png",
        "https://jalammar.github.io/images/t/transformer_decoding_1.gif",
        "https://jalammar.github.io/images/t/transformer_self_attention_vectors.png",
        "https://jalammar.github.io/images/t/transformer_decoder_output_softmax.png",
        "https://jalammar.github.io/images/t/self-attention_softmax.png",
        "https://jalammar.github.io/images/t/Transformer_decoder.png",
        "https://jalammar.github.io/images/t/The_transformer_encoders_decoders.png",
        "https://jalammar.github.io/images/t/vocabulary.png",
        "https://jalammar.github.io/images/t/transformer_self-attention_visualization.png",
        "https://jalammar.github.io/images/t/output_target_probability_distributions.png",
        "https://jalammar.github.io/images/t/attention-is-all-you-need-positional-encoding.png",
        "https://jalammar.github.io/images/t/transformer_attention_heads_qkv.png",
        "https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png",
        "https://jalammar.github.io/images/t/the_transformer_3.png",
        "https://jalammar.github.io/images/t/output_trained_model_probability_distributions.png",
        "https://jalammar.github.io/images/t/transformer_attention_heads_z.png",
        "https://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png",
        "https://jalammar.github.io/images/t/transformer_decoding_2.gif",
        "https://jalammar.github.io/images/t/embeddings.png",
      ],
    },
  },
  {
    heading: "The Transformer: A Deep Dive into Attention",
    content:
      "This article provides a detailed explanation of the Transformer model, a powerful architecture that utilizes attention mechanisms for efficient and accurate machine translation.\n\n",
    metadata: {
      external_references: {
        "": "",
        Press: "pressinquiries@medium.com?source=post_page-----4ac19b67fb4d--------------------------------",
      },
      sources: [
        "https://towardsdatascience.com/test-time-augmentation-tta-and-how-to-perform-it-with-keras-4ac19b67fb4d",
        "https://jalammar.github.io/illustrated-transformer/",
      ],
      imgs: [
        "",
        "https://jalammar.github.io/images/t/self-attention-output.png",
        "https://jalammar.github.io/images/t/transformer_positional_encoding_large_example.png",
        "https://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png",
        "https://miro.medium.com/v2/resize:fill:64:64/1*CJe3891yB1A1mzMdqemkdg.jpeg",
        "https://jalammar.github.io/images/t/The_transformer_encoder_decoder_stack.png",
        "https://jalammar.github.io/images/t/transformer_attention_heads_weight_matrix_o.png",
        "https://jalammar.github.io/images/t/transformer_self-attention_visualization_3.png",
        "https://jalammar.github.io/images/t/transformer_self-attention_visualization_2.png",
        "https://jalammar.github.io/images/t/encoder_with_tensors.png",
        "https://jalammar.github.io/images/t/encoder_with_tensors_2.png",
        "https://miro.medium.com/v2/resize:fill:144:144/2*DW37pyGS3g1xXe5_wOB1nw.png",
        "https://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png",
        "https://jalammar.github.io/images/t/transformer_self_attention_score.png",
        "https://jalammar.github.io/images/t/transformer_resideual_layer_norm_2.png",
        "https://jalammar.github.io/images/t/one-hot-vocabulary-example.png",
        "https://jalammar.github.io/images/t/self-attention-matrix-calculation.png",
        "https://jalammar.github.io/images/t/transformer_positional_encoding_vectors.png",
        "https://jalammar.github.io/images/t/transformer_logits_output_and_label.png",
        "https://jalammar.github.io/images/t/transformer_positional_encoding_example.png",
        "https://jalammar.github.io/images/t/Transformer_encoder.png",
        "https://jalammar.github.io/images/t/transformer_decoding_1.gif",
        "https://jalammar.github.io/images/t/transformer_self_attention_vectors.png",
        "https://jalammar.github.io/images/t/transformer_decoder_output_softmax.png",
        "https://jalammar.github.io/images/t/self-attention_softmax.png",
        "https://jalammar.github.io/images/t/Transformer_decoder.png",
        "https://jalammar.github.io/images/t/The_transformer_encoders_decoders.png",
        "https://jalammar.github.io/images/t/vocabulary.png",
        "https://jalammar.github.io/images/t/transformer_self-attention_visualization.png",
        "https://jalammar.github.io/images/t/output_target_probability_distributions.png",
        "https://jalammar.github.io/images/t/attention-is-all-you-need-positional-encoding.png",
        "https://jalammar.github.io/images/t/transformer_attention_heads_qkv.png",
        "https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png",
        "https://jalammar.github.io/images/t/the_transformer_3.png",
        "https://jalammar.github.io/images/t/output_trained_model_probability_distributions.png",
        "https://jalammar.github.io/images/t/transformer_attention_heads_z.png",
        "https://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png",
        "https://jalammar.github.io/images/t/transformer_decoding_2.gif",
        "https://jalammar.github.io/images/t/embeddings.png",
      ],
    },
  },
  {
    heading: "The Transformer: A Deep Dive into Attention",
    content:
      "### Overview\n\n* *Purpose:* The Transformer model is designed to improve the speed and performance of neural machine translation.\n* *Key Features:\n    * **Attention:* Allows the model to focus on relevant parts of the input sentence.\n    * *Parallelization:* Enables efficient training by allowing parallel processing of different parts of the input sequence.\n* *Advantages:*\n    * Outperforms traditional sequence-to-sequence models in specific tasks.\n    * Highly parallelizable, making it suitable for large-scale training.\n\n",
    metadata: {
      external_references: {
        "": "",
        Press: "pressinquiries@medium.com?source=post_page-----4ac19b67fb4d--------------------------------",
      },
      sources: [
        "https://towardsdatascience.com/test-time-augmentation-tta-and-how-to-perform-it-with-keras-4ac19b67fb4d",
        "https://jalammar.github.io/illustrated-transformer/",
      ],
      imgs: [
        "",
        "https://jalammar.github.io/images/t/self-attention-output.png",
        "https://jalammar.github.io/images/t/transformer_positional_encoding_large_example.png",
        "https://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png",
        "https://miro.medium.com/v2/resize:fill:64:64/1*CJe3891yB1A1mzMdqemkdg.jpeg",
        "https://jalammar.github.io/images/t/The_transformer_encoder_decoder_stack.png",
        "https://jalammar.github.io/images/t/transformer_attention_heads_weight_matrix_o.png",
        "https://jalammar.github.io/images/t/transformer_self-attention_visualization_3.png",
        "https://jalammar.github.io/images/t/transformer_self-attention_visualization_2.png",
        "https://jalammar.github.io/images/t/encoder_with_tensors.png",
        "https://jalammar.github.io/images/t/encoder_with_tensors_2.png",
        "https://miro.medium.com/v2/resize:fill:144:144/2*DW37pyGS3g1xXe5_wOB1nw.png",
        "https://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png",
        "https://jalammar.github.io/images/t/transformer_self_attention_score.png",
        "https://jalammar.github.io/images/t/transformer_resideual_layer_norm_2.png",
        "https://jalammar.github.io/images/t/one-hot-vocabulary-example.png",
        "https://jalammar.github.io/images/t/self-attention-matrix-calculation.png",
        "https://jalammar.github.io/images/t/transformer_positional_encoding_vectors.png",
        "https://jalammar.github.io/images/t/transformer_logits_output_and_label.png",
        "https://jalammar.github.io/images/t/transformer_positional_encoding_example.png",
        "https://jalammar.github.io/images/t/Transformer_encoder.png",
        "https://jalammar.github.io/images/t/transformer_decoding_1.gif",
        "https://jalammar.github.io/images/t/transformer_self_attention_vectors.png",
        "https://jalammar.github.io/images/t/transformer_decoder_output_softmax.png",
        "https://jalammar.github.io/images/t/self-attention_softmax.png",
        "https://jalammar.github.io/images/t/Transformer_decoder.png",
        "https://jalammar.github.io/images/t/The_transformer_encoders_decoders.png",
        "https://jalammar.github.io/images/t/vocabulary.png",
        "https://jalammar.github.io/images/t/transformer_self-attention_visualization.png",
        "https://jalammar.github.io/images/t/output_target_probability_distributions.png",
        "https://jalammar.github.io/images/t/attention-is-all-you-need-positional-encoding.png",
        "https://jalammar.github.io/images/t/transformer_attention_heads_qkv.png",
        "https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png",
        "https://jalammar.github.io/images/t/the_transformer_3.png",
        "https://jalammar.github.io/images/t/output_trained_model_probability_distributions.png",
        "https://jalammar.github.io/images/t/transformer_attention_heads_z.png",
        "https://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png",
        "https://jalammar.github.io/images/t/transformer_decoding_2.gif",
        "https://jalammar.github.io/images/t/embeddings.png",
      ],
    },
  },
  {
    heading: "The Transformer: A Deep Dive into Attention",
    content:
      "### Model Architecture\n\n* *Encoding Component:\n    * Stack of encoders (typically 6).\n    * Each encoder consists of:\n        * **Self-attention layer:* Allows the model to attend to different parts of the input sequence.\n        * *Feed-forward layer:* Applies a non-linear transformation to the input.\n* *Decoding Component:\n    * Stack of decoders (same number as encoders).\n    * Each decoder consists of:\n        * **Masked self-attention layer:* Prevents the decoder from attending to future words in the output sequence.\n        * *Encoder-decoder attention layer:* Allows the decoder to attend to relevant parts of the encoded input.\n        * *Feed-forward layer:* Applies a non-linear transformation to the input.\n\n",
    metadata: {
      external_references: {
        "": "",
        Press: "pressinquiries@medium.com?source=post_page-----4ac19b67fb4d--------------------------------",
      },
      sources: [
        "https://towardsdatascience.com/test-time-augmentation-tta-and-how-to-perform-it-with-keras-4ac19b67fb4d",
        "https://jalammar.github.io/illustrated-transformer/",
      ],
      imgs: [
        "",
        "https://jalammar.github.io/images/t/self-attention-output.png",
        "https://jalammar.github.io/images/t/transformer_positional_encoding_large_example.png",
        "https://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png",
        "https://miro.medium.com/v2/resize:fill:64:64/1*CJe3891yB1A1mzMdqemkdg.jpeg",
        "https://jalammar.github.io/images/t/The_transformer_encoder_decoder_stack.png",
        "https://jalammar.github.io/images/t/transformer_attention_heads_weight_matrix_o.png",
        "https://jalammar.github.io/images/t/transformer_self-attention_visualization_3.png",
        "https://jalammar.github.io/images/t/transformer_self-attention_visualization_2.png",
        "https://jalammar.github.io/images/t/encoder_with_tensors.png",
        "https://jalammar.github.io/images/t/encoder_with_tensors_2.png",
        "https://miro.medium.com/v2/resize:fill:144:144/2*DW37pyGS3g1xXe5_wOB1nw.png",
        "https://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png",
        "https://jalammar.github.io/images/t/transformer_self_attention_score.png",
        "https://jalammar.github.io/images/t/transformer_resideual_layer_norm_2.png",
        "https://jalammar.github.io/images/t/one-hot-vocabulary-example.png",
        "https://jalammar.github.io/images/t/self-attention-matrix-calculation.png",
        "https://jalammar.github.io/images/t/transformer_positional_encoding_vectors.png",
        "https://jalammar.github.io/images/t/transformer_logits_output_and_label.png",
        "https://jalammar.github.io/images/t/transformer_positional_encoding_example.png",
        "https://jalammar.github.io/images/t/Transformer_encoder.png",
        "https://jalammar.github.io/images/t/transformer_decoding_1.gif",
        "https://jalammar.github.io/images/t/transformer_self_attention_vectors.png",
        "https://jalammar.github.io/images/t/transformer_decoder_output_softmax.png",
        "https://jalammar.github.io/images/t/self-attention_softmax.png",
        "https://jalammar.github.io/images/t/Transformer_decoder.png",
        "https://jalammar.github.io/images/t/The_transformer_encoders_decoders.png",
        "https://jalammar.github.io/images/t/vocabulary.png",
        "https://jalammar.github.io/images/t/transformer_self-attention_visualization.png",
        "https://jalammar.github.io/images/t/output_target_probability_distributions.png",
        "https://jalammar.github.io/images/t/attention-is-all-you-need-positional-encoding.png",
        "https://jalammar.github.io/images/t/transformer_attention_heads_qkv.png",
        "https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png",
        "https://jalammar.github.io/images/t/the_transformer_3.png",
        "https://jalammar.github.io/images/t/output_trained_model_probability_distributions.png",
        "https://jalammar.github.io/images/t/transformer_attention_heads_z.png",
        "https://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png",
        "https://jalammar.github.io/images/t/transformer_decoding_2.gif",
        "https://jalammar.github.io/images/t/embeddings.png",
      ],
    },
  },
  {
    heading: "The Transformer: A Deep Dive into Attention",
    content:
      "### Attention Mechanism\n\n* *Self-attention:* Allows the model to understand the relationships between words in a sentence.\n* *Multi-headed attention:* Enhances the model's ability to focus on different positions and representation subspaces.\n* *Positional encoding:* Provides information about the position of each word in the sequence.\n\n",
    metadata: {
      external_references: {
        "": "",
        Press: "pressinquiries@medium.com?source=post_page-----4ac19b67fb4d--------------------------------",
      },
      sources: [
        "https://towardsdatascience.com/test-time-augmentation-tta-and-how-to-perform-it-with-keras-4ac19b67fb4d",
        "https://jalammar.github.io/illustrated-transformer/",
      ],
      imgs: [
        "",
        "https://jalammar.github.io/images/t/self-attention-output.png",
        "https://jalammar.github.io/images/t/transformer_positional_encoding_large_example.png",
        "https://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png",
        "https://miro.medium.com/v2/resize:fill:64:64/1*CJe3891yB1A1mzMdqemkdg.jpeg",
        "https://jalammar.github.io/images/t/The_transformer_encoder_decoder_stack.png",
        "https://jalammar.github.io/images/t/transformer_attention_heads_weight_matrix_o.png",
        "https://jalammar.github.io/images/t/transformer_self-attention_visualization_3.png",
        "https://jalammar.github.io/images/t/transformer_self-attention_visualization_2.png",
        "https://jalammar.github.io/images/t/encoder_with_tensors.png",
        "https://jalammar.github.io/images/t/encoder_with_tensors_2.png",
        "https://miro.medium.com/v2/resize:fill:144:144/2*DW37pyGS3g1xXe5_wOB1nw.png",
        "https://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png",
        "https://jalammar.github.io/images/t/transformer_self_attention_score.png",
        "https://jalammar.github.io/images/t/transformer_resideual_layer_norm_2.png",
        "https://jalammar.github.io/images/t/one-hot-vocabulary-example.png",
        "https://jalammar.github.io/images/t/self-attention-matrix-calculation.png",
        "https://jalammar.github.io/images/t/transformer_positional_encoding_vectors.png",
        "https://jalammar.github.io/images/t/transformer_logits_output_and_label.png",
        "https://jalammar.github.io/images/t/transformer_positional_encoding_example.png",
        "https://jalammar.github.io/images/t/Transformer_encoder.png",
        "https://jalammar.github.io/images/t/transformer_decoding_1.gif",
        "https://jalammar.github.io/images/t/transformer_self_attention_vectors.png",
        "https://jalammar.github.io/images/t/transformer_decoder_output_softmax.png",
        "https://jalammar.github.io/images/t/self-attention_softmax.png",
        "https://jalammar.github.io/images/t/Transformer_decoder.png",
        "https://jalammar.github.io/images/t/The_transformer_encoders_decoders.png",
        "https://jalammar.github.io/images/t/vocabulary.png",
        "https://jalammar.github.io/images/t/transformer_self-attention_visualization.png",
        "https://jalammar.github.io/images/t/output_target_probability_distributions.png",
        "https://jalammar.github.io/images/t/attention-is-all-you-need-positional-encoding.png",
        "https://jalammar.github.io/images/t/transformer_attention_heads_qkv.png",
        "https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png",
        "https://jalammar.github.io/images/t/the_transformer_3.png",
        "https://jalammar.github.io/images/t/output_trained_model_probability_distributions.png",
        "https://jalammar.github.io/images/t/transformer_attention_heads_z.png",
        "https://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png",
        "https://jalammar.github.io/images/t/transformer_decoding_2.gif",
        "https://jalammar.github.io/images/t/embeddings.png",
      ],
    },
  },
  {
    heading: "The Transformer: A Deep Dive into Attention",
    content:
      '### Training the Transformer\n\n* *Output vocabulary:* The model learns a set of unique words that it can output.\n* *One-hot encoding:* Each word in the vocabulary is represented by a vector with a single "1" and all other values set to "0".\n* *Cross-entropy loss:* Measures the difference between the predicted probability distribution and the actual probability distribution of the target word.\n* *Beam search:* A decoding algorithm that explores multiple possible translations and selects the best one.\n\n',
    metadata: {
      external_references: {
        "": "",
        Press: "pressinquiries@medium.com?source=post_page-----4ac19b67fb4d--------------------------------",
      },
      sources: [
        "https://towardsdatascience.com/test-time-augmentation-tta-and-how-to-perform-it-with-keras-4ac19b67fb4d",
        "https://jalammar.github.io/illustrated-transformer/",
      ],
      imgs: [
        "",
        "https://jalammar.github.io/images/t/self-attention-output.png",
        "https://jalammar.github.io/images/t/transformer_positional_encoding_large_example.png",
        "https://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png",
        "https://miro.medium.com/v2/resize:fill:64:64/1*CJe3891yB1A1mzMdqemkdg.jpeg",
        "https://jalammar.github.io/images/t/The_transformer_encoder_decoder_stack.png",
        "https://jalammar.github.io/images/t/transformer_attention_heads_weight_matrix_o.png",
        "https://jalammar.github.io/images/t/transformer_self-attention_visualization_3.png",
        "https://jalammar.github.io/images/t/transformer_self-attention_visualization_2.png",
        "https://jalammar.github.io/images/t/encoder_with_tensors.png",
        "https://jalammar.github.io/images/t/encoder_with_tensors_2.png",
        "https://miro.medium.com/v2/resize:fill:144:144/2*DW37pyGS3g1xXe5_wOB1nw.png",
        "https://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png",
        "https://jalammar.github.io/images/t/transformer_self_attention_score.png",
        "https://jalammar.github.io/images/t/transformer_resideual_layer_norm_2.png",
        "https://jalammar.github.io/images/t/one-hot-vocabulary-example.png",
        "https://jalammar.github.io/images/t/self-attention-matrix-calculation.png",
        "https://jalammar.github.io/images/t/transformer_positional_encoding_vectors.png",
        "https://jalammar.github.io/images/t/transformer_logits_output_and_label.png",
        "https://jalammar.github.io/images/t/transformer_positional_encoding_example.png",
        "https://jalammar.github.io/images/t/Transformer_encoder.png",
        "https://jalammar.github.io/images/t/transformer_decoding_1.gif",
        "https://jalammar.github.io/images/t/transformer_self_attention_vectors.png",
        "https://jalammar.github.io/images/t/transformer_decoder_output_softmax.png",
        "https://jalammar.github.io/images/t/self-attention_softmax.png",
        "https://jalammar.github.io/images/t/Transformer_decoder.png",
        "https://jalammar.github.io/images/t/The_transformer_encoders_decoders.png",
        "https://jalammar.github.io/images/t/vocabulary.png",
        "https://jalammar.github.io/images/t/transformer_self-attention_visualization.png",
        "https://jalammar.github.io/images/t/output_target_probability_distributions.png",
        "https://jalammar.github.io/images/t/attention-is-all-you-need-positional-encoding.png",
        "https://jalammar.github.io/images/t/transformer_attention_heads_qkv.png",
        "https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png",
        "https://jalammar.github.io/images/t/the_transformer_3.png",
        "https://jalammar.github.io/images/t/output_trained_model_probability_distributions.png",
        "https://jalammar.github.io/images/t/transformer_attention_heads_z.png",
        "https://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png",
        "https://jalammar.github.io/images/t/transformer_decoding_2.gif",
        "https://jalammar.github.io/images/t/embeddings.png",
      ],
    },
  },
  {
    heading: "The Transformer: A Deep Dive into Attention",
    content:
      "### Conclusion\n\nThe Transformer model is a powerful and efficient architecture that has revolutionized machine translation. Its use of attention mechanisms and parallelization capabilities make it a highly effective tool for natural language processing tasks.\n\n",
    metadata: {
      external_references: {
        "": "",
        Press: "pressinquiries@medium.com?source=post_page-----4ac19b67fb4d--------------------------------",
      },
      sources: [
        "https://towardsdatascience.com/test-time-augmentation-tta-and-how-to-perform-it-with-keras-4ac19b67fb4d",
        "https://jalammar.github.io/illustrated-transformer/",
      ],
      imgs: [
        "",
        "https://jalammar.github.io/images/t/self-attention-output.png",
        "https://jalammar.github.io/images/t/transformer_positional_encoding_large_example.png",
        "https://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png",
        "https://miro.medium.com/v2/resize:fill:64:64/1*CJe3891yB1A1mzMdqemkdg.jpeg",
        "https://jalammar.github.io/images/t/The_transformer_encoder_decoder_stack.png",
        "https://jalammar.github.io/images/t/transformer_attention_heads_weight_matrix_o.png",
        "https://jalammar.github.io/images/t/transformer_self-attention_visualization_3.png",
        "https://jalammar.github.io/images/t/transformer_self-attention_visualization_2.png",
        "https://jalammar.github.io/images/t/encoder_with_tensors.png",
        "https://jalammar.github.io/images/t/encoder_with_tensors_2.png",
        "https://miro.medium.com/v2/resize:fill:144:144/2*DW37pyGS3g1xXe5_wOB1nw.png",
        "https://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png",
        "https://jalammar.github.io/images/t/transformer_self_attention_score.png",
        "https://jalammar.github.io/images/t/transformer_resideual_layer_norm_2.png",
        "https://jalammar.github.io/images/t/one-hot-vocabulary-example.png",
        "https://jalammar.github.io/images/t/self-attention-matrix-calculation.png",
        "https://jalammar.github.io/images/t/transformer_positional_encoding_vectors.png",
        "https://jalammar.github.io/images/t/transformer_logits_output_and_label.png",
        "https://jalammar.github.io/images/t/transformer_positional_encoding_example.png",
        "https://jalammar.github.io/images/t/Transformer_encoder.png",
        "https://jalammar.github.io/images/t/transformer_decoding_1.gif",
        "https://jalammar.github.io/images/t/transformer_self_attention_vectors.png",
        "https://jalammar.github.io/images/t/transformer_decoder_output_softmax.png",
        "https://jalammar.github.io/images/t/self-attention_softmax.png",
        "https://jalammar.github.io/images/t/Transformer_decoder.png",
        "https://jalammar.github.io/images/t/The_transformer_encoders_decoders.png",
        "https://jalammar.github.io/images/t/vocabulary.png",
        "https://jalammar.github.io/images/t/transformer_self-attention_visualization.png",
        "https://jalammar.github.io/images/t/output_target_probability_distributions.png",
        "https://jalammar.github.io/images/t/attention-is-all-you-need-positional-encoding.png",
        "https://jalammar.github.io/images/t/transformer_attention_heads_qkv.png",
        "https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png",
        "https://jalammar.github.io/images/t/the_transformer_3.png",
        "https://jalammar.github.io/images/t/output_trained_model_probability_distributions.png",
        "https://jalammar.github.io/images/t/transformer_attention_heads_z.png",
        "https://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png",
        "https://jalammar.github.io/images/t/transformer_decoding_2.gif",
        "https://jalammar.github.io/images/t/embeddings.png",
      ],
    },
  },
  {
    heading: "The Transformer: A Deep Dive into Attention",
    content:
      "### Further Resources\n\n* *Attention Is All You Need paper:* Provides a detailed explanation of the Transformer model.\n* *Transformer blog post:* Offers a high-level overview of the Transformer architecture.\n* *Tensor2Tensor announcement:* Introduces the Tensor2Tensor library, which includes a TensorFlow implementation of the Transformer.\n* *Łukasz Kaiser’s talk:* Provides a walkthrough of the Transformer model and its details.\n* *Jupyter Notebook:* Allows users to interact with a trained Transformer model.\n\n",
    metadata: {
      external_references: {
        "": "",
        Press: "pressinquiries@medium.com?source=post_page-----4ac19b67fb4d--------------------------------",
      },
      sources: [
        "https://towardsdatascience.com/test-time-augmentation-tta-and-how-to-perform-it-with-keras-4ac19b67fb4d",
        "https://jalammar.github.io/illustrated-transformer/",
      ],
      imgs: [
        "",
        "https://jalammar.github.io/images/t/self-attention-output.png",
        "https://jalammar.github.io/images/t/transformer_positional_encoding_large_example.png",
        "https://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png",
        "https://miro.medium.com/v2/resize:fill:64:64/1*CJe3891yB1A1mzMdqemkdg.jpeg",
        "https://jalammar.github.io/images/t/The_transformer_encoder_decoder_stack.png",
        "https://jalammar.github.io/images/t/transformer_attention_heads_weight_matrix_o.png",
        "https://jalammar.github.io/images/t/transformer_self-attention_visualization_3.png",
        "https://jalammar.github.io/images/t/transformer_self-attention_visualization_2.png",
        "https://jalammar.github.io/images/t/encoder_with_tensors.png",
        "https://jalammar.github.io/images/t/encoder_with_tensors_2.png",
        "https://miro.medium.com/v2/resize:fill:144:144/2*DW37pyGS3g1xXe5_wOB1nw.png",
        "https://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png",
        "https://jalammar.github.io/images/t/transformer_self_attention_score.png",
        "https://jalammar.github.io/images/t/transformer_resideual_layer_norm_2.png",
        "https://jalammar.github.io/images/t/one-hot-vocabulary-example.png",
        "https://jalammar.github.io/images/t/self-attention-matrix-calculation.png",
        "https://jalammar.github.io/images/t/transformer_positional_encoding_vectors.png",
        "https://jalammar.github.io/images/t/transformer_logits_output_and_label.png",
        "https://jalammar.github.io/images/t/transformer_positional_encoding_example.png",
        "https://jalammar.github.io/images/t/Transformer_encoder.png",
        "https://jalammar.github.io/images/t/transformer_decoding_1.gif",
        "https://jalammar.github.io/images/t/transformer_self_attention_vectors.png",
        "https://jalammar.github.io/images/t/transformer_decoder_output_softmax.png",
        "https://jalammar.github.io/images/t/self-attention_softmax.png",
        "https://jalammar.github.io/images/t/Transformer_decoder.png",
        "https://jalammar.github.io/images/t/The_transformer_encoders_decoders.png",
        "https://jalammar.github.io/images/t/vocabulary.png",
        "https://jalammar.github.io/images/t/transformer_self-attention_visualization.png",
        "https://jalammar.github.io/images/t/output_target_probability_distributions.png",
        "https://jalammar.github.io/images/t/attention-is-all-you-need-positional-encoding.png",
        "https://jalammar.github.io/images/t/transformer_attention_heads_qkv.png",
        "https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png",
        "https://jalammar.github.io/images/t/the_transformer_3.png",
        "https://jalammar.github.io/images/t/output_trained_model_probability_distributions.png",
        "https://jalammar.github.io/images/t/transformer_attention_heads_z.png",
        "https://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png",
        "https://jalammar.github.io/images/t/transformer_decoding_2.gif",
        "https://jalammar.github.io/images/t/embeddings.png",
      ],
    },
  },
  {
    heading: "The Transformer: A Deep Dive into Attention",
    content:
      "### Related Work\n\n* *Depthwise Separable Convolutions for Neural Machine Translation\n *One Model To Learn Them All\n *Discrete Autoencoders for Sequence Models\n *Generating Wikipedia by Summarizing Long Sequences\n *Image Transformer\n *Training Tips for the Transformer Model\n *Self-Attention with Relative Position Representations\n *Fast Decoding in Sequence Models using Discrete Latent Variables\n *Adafactor: Adaptive Learning Rates with Sublinear Memory Cost*",
    metadata: {
      external_references: {
        "": "",
        Press: "pressinquiries@medium.com?source=post_page-----4ac19b67fb4d--------------------------------",
      },
      sources: [
        "https://towardsdatascience.com/test-time-augmentation-tta-and-how-to-perform-it-with-keras-4ac19b67fb4d",
        "https://jalammar.github.io/illustrated-transformer/",
      ],
      imgs: [
        "",
        "https://jalammar.github.io/images/t/self-attention-output.png",
        "https://jalammar.github.io/images/t/transformer_positional_encoding_large_example.png",
        "https://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png",
        "https://miro.medium.com/v2/resize:fill:64:64/1*CJe3891yB1A1mzMdqemkdg.jpeg",
        "https://jalammar.github.io/images/t/The_transformer_encoder_decoder_stack.png",
        "https://jalammar.github.io/images/t/transformer_attention_heads_weight_matrix_o.png",
        "https://jalammar.github.io/images/t/transformer_self-attention_visualization_3.png",
        "https://jalammar.github.io/images/t/transformer_self-attention_visualization_2.png",
        "https://jalammar.github.io/images/t/encoder_with_tensors.png",
        "https://jalammar.github.io/images/t/encoder_with_tensors_2.png",
        "https://miro.medium.com/v2/resize:fill:144:144/2*DW37pyGS3g1xXe5_wOB1nw.png",
        "https://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png",
        "https://jalammar.github.io/images/t/transformer_self_attention_score.png",
        "https://jalammar.github.io/images/t/transformer_resideual_layer_norm_2.png",
        "https://jalammar.github.io/images/t/one-hot-vocabulary-example.png",
        "https://jalammar.github.io/images/t/self-attention-matrix-calculation.png",
        "https://jalammar.github.io/images/t/transformer_positional_encoding_vectors.png",
        "https://jalammar.github.io/images/t/transformer_logits_output_and_label.png",
        "https://jalammar.github.io/images/t/transformer_positional_encoding_example.png",
        "https://jalammar.github.io/images/t/Transformer_encoder.png",
        "https://jalammar.github.io/images/t/transformer_decoding_1.gif",
        "https://jalammar.github.io/images/t/transformer_self_attention_vectors.png",
        "https://jalammar.github.io/images/t/transformer_decoder_output_softmax.png",
        "https://jalammar.github.io/images/t/self-attention_softmax.png",
        "https://jalammar.github.io/images/t/Transformer_decoder.png",
        "https://jalammar.github.io/images/t/The_transformer_encoders_decoders.png",
        "https://jalammar.github.io/images/t/vocabulary.png",
        "https://jalammar.github.io/images/t/transformer_self-attention_visualization.png",
        "https://jalammar.github.io/images/t/output_target_probability_distributions.png",
        "https://jalammar.github.io/images/t/attention-is-all-you-need-positional-encoding.png",
        "https://jalammar.github.io/images/t/transformer_attention_heads_qkv.png",
        "https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png",
        "https://jalammar.github.io/images/t/the_transformer_3.png",
        "https://jalammar.github.io/images/t/output_trained_model_probability_distributions.png",
        "https://jalammar.github.io/images/t/transformer_attention_heads_z.png",
        "https://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png",
        "https://jalammar.github.io/images/t/transformer_decoding_2.gif",
        "https://jalammar.github.io/images/t/embeddings.png",
      ],
    },
  },
  {
    heading: "Data Augmentation",
    content:
      "Data Augmentation is the process of randomly applying operations (rotation, zoom, shift, flips,…) to the input data. This ensures that the model is never shown the exact same example twice, forcing it to learn more general features about the classes it needs to recognize.",
    metadata: {
      external_references: {
        "": "",
      },
      sources: [
        "https://towardsdatascience.com/test-time-augmentation-tta-and-how-to-perform-it-with-keras-4ac19b67fb4d",
      ],
      imgs: [""],
    },
  },
  {
    heading: "Test Time Augmentation",
    content:
      'Similar to Data Augmentation on the training set, Test Time Augmentation performs random modifications to the test images. Instead of showing the "clean" images only once to the trained model, we show it the augmented images several times. The predictions for each corresponding image are then averaged to arrive at the final guess.\n\n',
    metadata: {
      external_references: {
        "": "",
      },
      sources: [
        "https://towardsdatascience.com/test-time-augmentation-tta-and-how-to-perform-it-with-keras-4ac19b67fb4d",
      ],
      imgs: [""],
    },
  },
  {
    heading: "Test Time Augmentation",
    content:
      "### Example of Test Time Augmentation\n\nImagine presenting 5 slightly modified versions of the same image to a network and asking it to predict the class of each. The corresponding predictions might look like this:\n\n* *Image 1:* Prediction A\n* *Image 2:* Prediction B\n* *Image 3:* Prediction A\n* *Image 4:* Prediction A\n* *Image 5:* Prediction B\n\n",
    metadata: {
      external_references: {
        "": "",
      },
      sources: [
        "https://towardsdatascience.com/test-time-augmentation-tta-and-how-to-perform-it-with-keras-4ac19b67fb4d",
      ],
      imgs: [""],
    },
  },
  {
    heading: "Test Time Augmentation",
    content:
      "### Benefits of Test Time Augmentation\n\nTest Time Augmentation is particularly useful for test images that the model is unsure about. Even if those 5 images seem very similar to you, the model might perceive them as very different, as evidenced by its predictions.\n\n",
    metadata: {
      external_references: {
        "": "",
      },
      sources: [
        "https://towardsdatascience.com/test-time-augmentation-tta-and-how-to-perform-it-with-keras-4ac19b67fb4d",
      ],
      imgs: [""],
    },
  },
  {
    heading: "Test Time Augmentation",
    content:
      "### Implementing Test Time Augmentation\n\nTo implement Test Time Augmentation, you can reuse the same Data Generator used for training and apply it to validation images.\n\n",
    metadata: {
      external_references: {
        "": "",
      },
      sources: [
        "https://towardsdatascience.com/test-time-augmentation-tta-and-how-to-perform-it-with-keras-4ac19b67fb4d",
      ],
      imgs: [""],
    },
  },
  {
    heading: "Test Time Augmentation",
    content:
      "### Training with Test Time Augmentation\n\nAfter applying Test Time Augmentation, you can train the network for a few epochs. The final accuracy of the model on the validation images can then be evaluated.",
    metadata: {
      external_references: {
        "": "",
      },
      sources: [
        "https://towardsdatascience.com/test-time-augmentation-tta-and-how-to-perform-it-with-keras-4ac19b67fb4d",
      ],
      imgs: [""],
    },
  },
];

interface Question {
  question: string;
  type: "text" | "single" | "multi";
  options?: string[];
  id: string;
  answer: string;
}

const EVALUATE_AI: Question[] = [
  {
    question: "What is the primary purpose of data augmentation?",
    options: [
      "To increase the size of a training dataset",
      "To improve model accuracy on unseen data",
      "To reduce overfitting",
      "All of the above",
    ],
    answer: "All of the above",
    type: "single",
    id: "0",
  },
  {
    question: "Which of the following is NOT an example of data augmentation for image data?",
    options: ["Flipping", "Rotating", "Adding noise", "Colorizing"],
    answer: "Colorizing",
    type: "single",
    id: "1",
  },
  {
    question: "What is the main benefit of test time augmentation (TTA)?",
    options: [
      "It improves model robustness to slight changes in input data",
      "It reduces overfitting",
      "It enhances model accuracy by averaging predictions from multiple augmented versions of the test data",
      "It increases the size of the test dataset",
    ],
    answer: "It enhances model accuracy by averaging predictions from multiple augmented versions of the test data",
    type: "single",
    id: "2",
  },
  {
    question: "Which of the following augmentation techniques is generally beneficial for CIFAR10 images?",
    options: ["Random flips", "Large rotations", "Horizontal flips", "Vertical flips"],
    answer: "Horizontal flips",
    type: "single",
    id: "3",
  },
  {
    question: "What is the primary function of the attention mechanism in the Transformer model?",
    options: [
      "To process input sequences in parallel",
      "To focus on relevant parts of the input sentence",
      "To generate the output sequence",
      "To encode the input sequence into a fixed-length vector",
    ],
    answer: "To focus on relevant parts of the input sentence",
    type: "single",
    id: "4",
  },
  {
    question:
      "Which component of the Transformer model is responsible for preventing the decoder from attending to future words in the output sequence?",
    options: [
      "Self-attention layer",
      "Masked self-attention layer",
      "Encoder-decoder attention layer",
      "Feed-forward layer",
    ],
    answer: "Masked self-attention layer",
    type: "single",
    id: "5",
  },
  {
    question: "What is the purpose of positional encoding in the Transformer model?",
    options: [
      "To provide information about the order of words in the sequence",
      "To encode the input sequence into a fixed-length vector",
      "To generate the output sequence",
      "To focus on relevant parts of the input sentence",
    ],
    answer: "To provide information about the order of words in the sequence",
    type: "single",
    id: "6",
  },
  {
    question: "What is the primary advantage of the Transformer model over traditional sequence-to-sequence models?",
    options: [
      "It is more efficient for processing long sequences",
      "It is more accurate for specific tasks",
      "It is more parallelizable",
      "All of the above",
    ],
    answer: "All of the above",
    type: "single",
    id: "7",
  },
  {
    question: "What is the primary purpose of Data Augmentation?",
    options: [
      "To increase the size of the training dataset.",
      "To improve the accuracy of the model on unseen data.",
      "To reduce the training time of the model.",
      "To prevent overfitting by exposing the model to diverse variations of the same data.",
    ],
    answer: "To prevent overfitting by exposing the model to diverse variations of the same data.",
    type: "single",
    id: "8",
  },
  {
    question: "How does Test Time Augmentation differ from Data Augmentation?",
    options: [
      "Test Time Augmentation is applied to the training dataset, while Data Augmentation is applied to the test dataset.",
      "Test Time Augmentation is applied to the test dataset, while Data Augmentation is applied to the training dataset.",
      "Test Time Augmentation uses a different set of augmentation operations than Data Augmentation.",
      "Test Time Augmentation is used to improve the model's performance on the training dataset, while Data Augmentation is used to improve the model's performance on the test dataset.",
    ],
    answer:
      "Test Time Augmentation is applied to the test dataset, while Data Augmentation is applied to the training dataset.",
    type: "single",
    id: "9",
  },
  {
    question: "What is the main benefit of Test Time Augmentation?",
    options: [
      "It reduces the computational cost of training the model.",
      "It improves the model's ability to generalize to unseen data.",
      "It helps to identify and correct errors in the training dataset.",
      "It makes the model more robust to noisy or corrupted data.",
    ],
    answer: "It improves the model's ability to generalize to unseen data.",
    type: "single",
    id: "10",
  },
  {
    question: "How are predictions made using Test Time Augmentation?",
    options: [
      "The model predicts the class of each augmented image individually, and the final prediction is based on the most frequent class.",
      "The model predicts the class of each augmented image individually, and the final prediction is based on the average of all predictions.",
      "The model predicts the class of the original image only, and the final prediction is based on the average of all predictions.",
      "The model predicts the class of the original image only, and the final prediction is based on the most frequent class.",
    ],
    answer:
      "The model predicts the class of each augmented image individually, and the final prediction is based on the average of all predictions.",
    type: "single",
    id: "11",
  },
  {
    question: "What is a common way to implement Test Time Augmentation?",
    options: [
      "Using a separate data generator specifically for test time augmentation.",
      "Reusing the same data generator used for training on the validation images.",
      "Manually applying augmentation operations to each test image.",
      "Using a pre-trained model that has already been trained with Test Time Augmentation.",
    ],
    answer: "Reusing the same data generator used for training on the validation images.",
    type: "single",
    id: "12",
  },
];

interface chat_ai {
  id: string;
  text: string;
  external_references: {};
  sources: string[];
  imgs: string[];
}

const CHAT_AI: chat_ai[] = [
  {
    id: "1",
    text: "Yoga for Lower Blood Pressure: A Comprehensive Guide\n\nNumerous studies have demonstrated the remarkable benefits of yoga in managing hypertension. By incorporating yoga into your routine, you can significantly reduce your blood pressure and improve your overall well-being.\n\n### How Yoga Works\n\n* **Relaxation and Stress Reduction:** Yoga techniques, such as deep breathing and meditation, help calm the mind and reduce stress, which are major contributors to high blood pressure.\n* **Improved Blood Vessel Flexibility:** Regular yoga practice can increase the flexibility of your blood vessels, allowing for better blood flow and reducing the strain on your heart.\n* **Enhanced Cardiovascular Health:** Yoga poses can strengthen your heart and improve its efficiency, leading to healthier blood pressure levels.\n\n### Recommended Yoga Poses for High Blood Pressure\n\n1. **Balasana (Child's Pose):**\n   * **Benefits:** Promotes relaxation, calms the mind, and stretches the hips and spine.\n   * **How to:** Kneel on the floor with your big toes touching. Sit back on your heels, extending your arms forward. Rest your forehead on the floor. Hold for 1-3 minutes.\n\n2. **Paschimottanasana (Seated Forward Bend):**\n   * **Benefits:** Calms the mind, stretches the hamstrings, and can help lower blood pressure.\n   * **How to:** Sit on the floor with your legs extended. Inhale and reach for your toes, keeping your back straight. Exhale and bend forward, reaching as far as you can without straining. Hold for 1-3 minutes.\n\n### Additional Tips\n\n* **Consistency is Key:** Practice yoga regularly to experience the full benefits.\n* **Listen to Your Body:** Avoid overexerting yourself.\n* **Consider a Yoga Class:** A guided yoga class can provide proper instruction and support.\n\nBy incorporating these yoga poses and techniques into your daily routine, you can effectively manage your blood pressure and enjoy a healthier, more balanced life. Remember to consult with your healthcare provider before starting any new exercise program.",
    external_references: {
      include: "https://www.heart.org/en/health-topics/high-blood-pressure/health-threats-from-high-blood-pressure",
      arrhythmia: "https://www.healthline.com/health/arrhythmia",
      "vision loss": "https://www.healthline.com/health/hypertensive-retinopathy",
      "sexual dysfunction":
        "https://www.healthline.com/health/high-blood-pressure-hypertension-linked-to-erectile-dysfunction",
      dementia: "https://www.healthline.com/health/dementia/vascular-dementia",
      including: "https://www.ncbi.nlm.nih.gov/books/NBK430839/",
      "decreased kidney function": "https://www.healthline.com/health/pregnancy/kidney-failure-in-pregnancy",
      preeclampsia: "https://www.healthline.com/health/gestational-hypertension-vs-preeclampsia",
    },
    sources: [
      "https://www.medicalnewstoday.com/articles/319506#causes",
      "https://www.yogajournal.com/poses/yoga-by-benefit/high-blood-pressure/yoga-for-high-blood-pressure/",
      "https://www.healthline.com/health/high-blood-pressure-hypertension",
    ],
    imgs: [
      "https://cdn.yogajournal.com/wp-content/uploads/2021/10/Childs-Pose_Andrew-Clark_1.jpg?width=730",
      "https://cdn.yogajournal.com/wp-content/uploads/2021/12/Bound-Angle-Pose-Mod-1_Andrew-Clark_1.jpg?width=730",
      "https://media.post.rvohealth.io/wp-content/uploads/sites/3/2023/11/raise_blood-_GettyImages1171000270_Header-1024x575.jpg",
    ],
  },
];

export { BLOG_AI, WATCH_AI, EVALUATE_AI, CHAT_AI };
